{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "\n",
    "import string\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = False # setting False saves the output files else not saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output log file\n",
    "if not trial:\n",
    "    try:\n",
    "        os.system(\"mkdir output_files\")\n",
    "    except:\n",
    "        print (\"Folder output_files already present\")\n",
    "\n",
    "    present_files = glob.glob(\"output_files/log_output_*.txt\")\n",
    "    log_file_name = \"output_files/log_output_\" + str(len(present_files) + 1) + \".txt\"\n",
    "    log_file = open(log_file_name,\"w\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_files = True\n",
    "\n",
    "if multi_files:\n",
    "    input_file = [\"first_5000.txt\",\"ML_input_5338.txt\"]\n",
    "else:\n",
    "    input_file = \"first_5000.txt\" # Input data containing smiles and label\n",
    "\n",
    "log_file.write(\"Used files \" + str(input_file) + \"\\n\")\n",
    "number_of_augmentation = 1 # Data augmentation multiplier\n",
    "train_percentage = 0.9 # Fraction to use for training (valida and test would be half of remaining data)\n",
    "Number_of_workers = 8 # Number of CPU threads to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rdkit warnings (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove rdkit warning\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_canonical_smiles(molecule):\n",
    "    try:\n",
    "        canonical_smile = Chem.MolToSmiles(Chem.MolFromSmiles(molecule))\n",
    "    except:\n",
    "        canonical_smile = False\n",
    "    return canonical_smile\n",
    "\n",
    "def get_cluster_count(y_count):\n",
    "    cluster_count = {}\n",
    "    for y in y_count:\n",
    "        if y not in cluster_count:\n",
    "            cluster_count[y] = 1\n",
    "        else:\n",
    "            cluster_count[y] +=1\n",
    "    return (cluster_count)\n",
    "\n",
    "def randomize_smiles(smiles,random_smiles=[],iteration=5):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        ans = list(range(m.GetNumAtoms()))\n",
    "        np.random.shuffle(ans)\n",
    "        nm = Chem.RenumberAtoms(m,ans)\n",
    "        out_smiles = (Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False))\n",
    "    except:\n",
    "        return (False)\n",
    "    \n",
    "    if out_smiles not in random_smiles:\n",
    "        return out_smiles\n",
    "    else:\n",
    "        iteration -= 1\n",
    "        if iteration > 0:\n",
    "            out_smiles = randomize_smiles(smiles,random_smiles,iteration)\n",
    "            return out_smiles\n",
    "        return (False)\n",
    "    \n",
    "def augment_smiles(count,iteration,smiles):\n",
    "    random_smiles = []\n",
    "    for i in range(count):\n",
    "        if smiles != None:\n",
    "            out_smiles = randomize_smiles(smiles,random_smiles,iteration=iteration)\n",
    "            if out_smiles:\n",
    "                random_smiles.append(out_smiles)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    return random_smiles\n",
    "\n",
    "def unpack_and_write_list(smiles,label,filename):\n",
    "    for entry in smiles:\n",
    "        if type(entry) == list:\n",
    "            unpack_and_write_list(entry,label,filename)\n",
    "        else:\n",
    "            filename.write(entry + \",\" + str(label) + \"\\n\")\n",
    "    \n",
    "def smiles_augmentation(df, N_rounds=1,iteration=5,data_set_type=\"train\"):\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(\"data\")\n",
    "        os.mkdir(\"data/classification\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    filename = \"data/classification/\" + str(data_set_type) + \"_aug_canonical_smiles.csv\"\n",
    "\n",
    "    aug_out = open(filename,\"w\")\n",
    "\n",
    "    aug_out.write(\"Smiles,Label\\n\")\n",
    "        \n",
    "    labels = []\n",
    "    for label in df.groupby('Label'):\n",
    "        labels.append(label[0])\n",
    "    \n",
    "    augmentation_list = []\n",
    "    if type(N_rounds) == list:\n",
    "        assert(len(N_rounds) == len(labels))\n",
    "        augmentation_list = N_rounds\n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            augmentation_list.append(N_rounds)\n",
    "        \n",
    "    for label,augmentation in zip(labels,augmentation_list):\n",
    "    \n",
    "        canonical_smiles = df[df['Label'] == label]['Smiles'].to_list()\n",
    "\n",
    "        p = Pool(Number_of_workers)\n",
    "        func = partial(augment_smiles, augmentation, iteration)\n",
    "        augmented_smiles = list(tqdm.tqdm(p.imap(func, canonical_smiles), total=len(canonical_smiles),leave=False))\n",
    "        p.close()\n",
    "    \n",
    "        print (\"Saving data for label = \" + str(label))\n",
    "\n",
    "        unpack_and_write_list(augmented_smiles,label,filename=aug_out)\n",
    "\n",
    "        unpack_and_write_list(canonical_smiles,label,filename=aug_out)\n",
    "        \n",
    "        print (\"Saved data for label = \" + str(label))\n",
    "        \n",
    "    aug_out.close()\n",
    "    \n",
    "    return (pd.read_csv(filename, header=0).sample(frac=1).reset_index(drop=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (9854, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCN(CC)C(=O)C1=CC(=C(C=C1)O)OC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=NC2=C(C(=N1)N)N=CN2C3C(C(C(O3)COP(=O)(O)OP(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1=CC(=CC(=C1)O)C(=O)O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1=CC=C(C=C1)CCNC2=NC=NC3=C2N=CN3C4C(C(C(O4)CO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OP(=[Se])(O)O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles  Label\n",
       "0                     CCN(CC)C(=O)C1=CC(=C(C=C1)O)OC      0\n",
       "1  C1=NC2=C(C(=N1)N)N=CN2C3C(C(C(O3)COP(=O)(O)OP(...      1\n",
       "2                             C1=CC(=CC(=C1)O)C(=O)O      2\n",
       "3  C1=CC=C(C=C1)CCNC2=NC=NC3=C2N=CN3C4C(C(C(O4)CO...      1\n",
       "4                                      OP(=[Se])(O)O      1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_files:\n",
    "    for i,input_filename in enumerate(input_file):\n",
    "        if i != 0:\n",
    "            quantmap_data2 = pd.read_csv(input_filename,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True) #,header=None)\n",
    "            quantmap_data = pd.concat([quantmap_data,quantmap_data2])\n",
    "        else:\n",
    "            quantmap_data = pd.read_csv(input_filename,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True) #,header=None)\n",
    "    del quantmap_data2\n",
    "else:\n",
    "    quantmap_data = pd.read_csv(input_file,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "print('Dataset:', quantmap_data.shape)\n",
    "quantmap_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Smiles\n",
      "Label        \n",
      "0        1919\n",
      "1        4662\n",
      "2        2508\n",
      "3         390\n",
      "4         375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smiles\n",
       "Label        \n",
       "0        1919\n",
       "1        4662\n",
       "2        2508\n",
       "3         390\n",
       "4         375"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (quantmap_data.groupby('Label').count())\n",
    "if not trial:\n",
    "    log_file.write(\"Class distribution before augmentation\\n\")\n",
    "    log_file.write(str(quantmap_data.groupby('Label').count()) + \"\\n\")\n",
    "    \n",
    "quantmap_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_df = quantmap_data.groupby('Label').count()\n",
    "label_count_list = []\n",
    "for entry in range(len(label_count_df)):\n",
    "    label_count_list.append(label_count_df.iloc[entry][0])\n",
    "\n",
    "augmentation_list = []\n",
    "max_value = max(label_count_list)\n",
    "for entry in label_count_list:\n",
    "    augmentation_list.append(int(max_value/entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 0\n",
      "Saved data for label = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 1\n",
      "Saved data for label = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 2\n",
      "Saved data for label = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 3\n",
      "Saved data for label = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 4\n",
      "Saved data for label = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "###\n",
    "augmentation_list = [entry*number_of_augmentation for entry in augmentation_list]\n",
    "iteration = 10000\n",
    "# Augmentation for training data\n",
    "quantmap_data = smiles_augmentation(quantmap_data,N_rounds=augmentation_list,iteration=iteration,data_set_type=\"all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Smiles\n",
      "Label        \n",
      "0        5749\n",
      "1        9323\n",
      "2        5016\n",
      "3        4569\n",
      "4        4680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smiles\n",
       "Label        \n",
       "0        5749\n",
       "1        9323\n",
       "2        5016\n",
       "3        4569\n",
       "4        4680"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (quantmap_data.groupby('Label').count())\n",
    "if not trial:\n",
    "    log_file.write(\"number of augmentation = \" + str(number_of_augmentation) + \"\\n\")\n",
    "    log_file.write(\"Class distribution after augmentation\\n\")\n",
    "    log_file.write(str(quantmap_data.groupby('Label').count()) + \"\\n\")\n",
    "    log_file.write(\"Train/valid split ratio = \" + str(train_percentage) + \"\\n\")\n",
    "    \n",
    "quantmap_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label = \"None\" #\"fingerprint\"\n",
    "class MolTokenizer():\n",
    "    def __init__(self, lang = 'en'):\n",
    "        self.lang = lang\n",
    "        \n",
    "    def tokenizer(self, output_label=None,smiles=None):\n",
    "        tokens = atomwise_tokenizer(smiles)\n",
    "        tokens.insert(0, \"<SOS>\") \n",
    "        tokens.append(\"<EOS>\") \n",
    "        if output_label != \"fingerprint\":\n",
    "            return tokens\n",
    "        else:\n",
    "            try:\n",
    "                fingerprint = smiles_fingerprint(smiles,\"morgan\", radius=2,bits=1024)\n",
    "                return tokens,fingerprint\n",
    "            except:\n",
    "                return None,None\n",
    "        \n",
    "    def add_special_cases(self, toks):\n",
    "        pass\n",
    "\n",
    "tok = MolTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make vocabulary\n",
    "\n",
    "def unpack_vocab_list(vocab_list,vocab_unpacked):\n",
    "    for entry in vocab_list:\n",
    "        if type(entry) == list:\n",
    "            unpack_vocab_list(entry,vocab_unpacked)\n",
    "        else:\n",
    "            vocab_unpacked.append(entry)\n",
    "            \n",
    "    return (vocab_unpacked)\n",
    "            \n",
    "def make_vocabulary(input_list):\n",
    "    p = Pool(Number_of_workers)\n",
    "    func = partial(tok.tokenizer, None)\n",
    "    vocab_list = list(tqdm.tqdm(p.imap(func, input_list), total=len(input_list),leave=False))\n",
    "    p.close()\n",
    "    vocab_unpacked = []\n",
    "    return (list(set(unpack_vocab_list(vocab_list,vocab_unpacked))))\n",
    "\n",
    "def make_word_index(vocab):\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    for i,entry in enumerate(vocab):\n",
    "        word_index[entry] = i\n",
    "        index_word[i] = entry\n",
    "    return (word_index,index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vocab_file = True\n",
    "\n",
    "if use_vocab_file:\n",
    "    en_vocab = open(\"en_vocab_.txt\",\"r\").read().strip(\"[]\").replace(\"'\", \"\").replace(\" \", \"\").split(\",\")\n",
    "    en_word_index,en_index_word = make_word_index(en_vocab)\n",
    "    \n",
    "else:\n",
    "    all_smiles = quantmap_data[\"Smiles\"].to_list()\n",
    "\n",
    "    en_vocab = [\"<PAD>\",\"<UNK>\"]\n",
    "    en_vocab.extend(make_vocabulary(all_smiles))\n",
    "    del all_smiles\n",
    "\n",
    "    ###\n",
    "    en_word_index,en_index_word = make_word_index(en_vocab)\n",
    "\n",
    "    vocab_output = open(\"en_vocab_.txt\",\"w\")\n",
    "    vocab_output.write(str(en_vocab))\n",
    "    vocab_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/anaconda3/envs/molpmofit/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "/home/jovyan/anaconda3/envs/molpmofit/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smiles\n",
       "Label        \n",
       "0        4569\n",
       "1        4680"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unbalanced data and shuffling\n",
    "balanced_data = quantmap_data [quantmap_data.Label != 0][quantmap_data.Label != 1][quantmap_data.Label != 2]\n",
    "balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
    "balanced_data[\"Label\"].replace({3: 0,4:1}, inplace=True)\n",
    "balanced_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trial:\n",
    "    log_file.write(\"Class distribution after balanced dataset\\n\")\n",
    "    log_file.write(str(balanced_data.groupby('Label').count()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count_list = np.array([entry for entry in balanced_data.groupby('Label').count()[\"Smiles\"]])\n",
    "class_weight = np.max(class_count_list)/class_count_list\n",
    "class_weight = torch.FloatTensor(class_weight).cuda()\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"Class weight for loss (balancing weights)= \" + str(class_weight) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_tokens(df):\n",
    "    \n",
    "    if output_label != \"fingerprint\": # Maintain the y from the data as label\n",
    "        labels = []\n",
    "        for label in df.groupby('Label'):\n",
    "            labels.append(label[0])\n",
    "            \n",
    "        x = []\n",
    "        y = []\n",
    "        p = Pool(Number_of_workers)\n",
    "        for label in labels:\n",
    "            smiles_list = df[df['Label'] == label]['Smiles'].to_list()\n",
    "            \n",
    "            \n",
    "            func = partial(tok.tokenizer, None)\n",
    "            tokens = list(tqdm.tqdm(p.imap(func, smiles_list), total=len(smiles_list),leave=False))\n",
    "            \n",
    "            #p = Pool(Number_of_workers)\n",
    "            #tokens = list(tqdm.tqdm(p.imap(tok.tokenizer, smiles_list), total=len(smiles_list),leave=False))\n",
    "            #p.close()\n",
    "            #if len(tokens) < 100:\n",
    "            for entry in tokens:\n",
    "                if  5 < len(entry) <= 150:\n",
    "                #break\n",
    "                    x.append(entry)\n",
    "                    y.append(label)\n",
    "            #x.extend(tokens)\n",
    "            #y.extend([label for i in range(len(tokens))])\n",
    "        p.close()\n",
    "            \n",
    "    else: # Return fingerprint as label for each object\n",
    "        x = []\n",
    "        y = []\n",
    "        smiles_list = df['Smiles'].to_list()\n",
    "        \n",
    "        func = partial(tok.tokenizer, output_label)\n",
    "        \n",
    "        for entry in smiles_list:\n",
    "            xout,yout = (func(entry))\n",
    "            if xout != None:\n",
    "                x.append(xout)\n",
    "                y.append([int(entry) for entry in yout])\n",
    "        \n",
    "        print (str(len(smiles_list)-len(x)) + \" incorrect smiles detected and deleted\"  )\n",
    "        \n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "x,y= convert_smiles_to_tokens(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenthwise_x = [len(entry) for entry in x if len(entry)]\n",
    "if not trial:\n",
    "    log_file.write(\"Data point with (5 < len(sequence) <= 150) = \" + str(len(lenthwise_x)) +\"/\"+ str(sum(class_count_list)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(lenthwise_x, density=True, bins=30)  # density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token_to_index(molecule):\n",
    "    idxs = []\n",
    "    for ch in molecule:\n",
    "        if ch in en_word_index:\n",
    "            idxs.append(en_word_index[ch])\n",
    "        else:\n",
    "            idxs.append(en_word_index[\"<UNK>\"])\n",
    "    return torch.tensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "x_indexed_token = []\n",
    "loop = tqdm.tqdm(x, total=len(x),leave=False)\n",
    "for entry in loop:\n",
    "    x_indexed_token.append(convert_token_to_index(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_test_percentage = (1 - train_percentage)/2\n",
    "\n",
    "data_to_use = balanced_data\n",
    "# Ratios\n",
    "train_ratio = int (len(data_to_use) * train_percentage)\n",
    "valid_ratio = train_ratio + int(len(data_to_use)*valid_test_percentage)\n",
    "test_ratio = valid_ratio + int(len(data_to_use)*valid_test_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make index to split into train and val set\n",
    "def make_index(len_data,train_ratio,valid_ratio,test_ratio):\n",
    "    \n",
    "    index = np.random.permutation(len_data)\n",
    "    \n",
    "    # Train index and val index\n",
    "    return (index[:train_ratio],index[train_ratio:valid_ratio],index[valid_ratio:test_ratio])\n",
    "\n",
    "train_index ,valid_index,test_index = make_index(len(data_to_use),train_ratio,valid_ratio,test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders by padding the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded = pad_sequence(x_indexed_token,batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "train_sample = SubsetRandomSampler(train_index)\n",
    "valid_sample = SubsetRandomSampler(valid_index)\n",
    "test_sample = SubsetRandomSampler(test_index)\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x_padded,torch.tensor(y))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                        sampler=train_sample)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                        sampler=valid_sample)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                        sampler=test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data):\n",
    "    \n",
    "    index = np.random.permutation(len(data))\n",
    "    \n",
    "    output_shuffled = []\n",
    "    for i in index:\n",
    "        output_shuffled.append(data[i])\n",
    "        \n",
    "    return (output_shuffled)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = shuffle_data([(entry,y[i]) for i,entry in enumerate(x_indexed_token) if i in train_index])\n",
    "valid_data = shuffle_data([(entry,y[i]) for i,entry in enumerate(x_indexed_token) if i in valid_index])\n",
    "test_data = shuffle_data([(entry,y[i]) for i,entry in enumerate(x_indexed_token) if i in test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset class for loading data.\n",
    "      This is where the data parsing happens.\n",
    "      This class is built with reusability in mind.\n",
    "      Arguments:\n",
    "      path (:obj:`str`):\n",
    "      Path to the data partition.\n",
    "      \"\"\"\n",
    "    \n",
    "    def __init__(self, data_tuple):\n",
    "\n",
    "        # Check if path exists.\n",
    "        #if not os.path.isdir(path):\n",
    "          # Raise error if path is invalid.\n",
    "          #raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "    \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        # Since the labels are defined by folders with data we loop \n",
    "        # through each label.\n",
    "        '''for label  in ['pos', 'neg']:\n",
    "            sentiment_path = os.path.join(path, label)\n",
    "\n",
    "            # Get all files from path.\n",
    "            files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n",
    "            # Go through each file and read its content.\n",
    "            for file_name in tqdm(files_names, desc=f'{label} Files'):\n",
    "                file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "                # Read content.\n",
    "                content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "                # Fix any unicode issues.\n",
    "                content = fix_text(content)\n",
    "                # Save content.\n",
    "                self.texts.append(content)\n",
    "                # Save labels.\n",
    "                self.labels.append(label)'''\n",
    "        for entry in data_tuple:\n",
    "            self.texts.append(entry[0])\n",
    "            self.labels.append(entry[1])\n",
    "        # Number of examples.\n",
    "        self.n_examples = len(self.labels)\n",
    "        return\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"When used `len` return the number of examples.\"\"\"\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Given an index return an example from the position.\n",
    "        Arguments:\n",
    "          item (:obj:`int`):\n",
    "              Index position to pick an example to return.\n",
    "        Returns:\n",
    "          :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed \n",
    "          to a model.\n",
    "        \"\"\"\n",
    "        return {'text':self.texts[item], 'label':self.labels[item]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MakeDataset(train_data)\n",
    "valid_dataset = MakeDataset(valid_data)\n",
    "test_dataset = MakeDataset(test_data)\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_dataset,\n",
    "    sort = False,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x['text']),\n",
    "    batch_size = batch_size,\n",
    "    device = device)\n",
    "\n",
    "valid_iterator = data.BucketIterator(\n",
    "    valid_dataset,\n",
    "    sort = False,\n",
    "    shuffle=True,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x['text']),\n",
    "    batch_size = batch_size,\n",
    "    device = device)\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_dataset,\n",
    "    sort = False,\n",
    "    shuffle=True,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x['text']),\n",
    "    batch_size = batch_size,\n",
    "    device = device)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"Batch size = \" + str(batch_size) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) #,padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p,batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N,seq_length) where N is batch size\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (N,seq_length, embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (N,seq_length, hidden_size)\n",
    "        \n",
    "        #cat = torch.cat(hidden[0],hidden[1],hidden[2],dim=2)\n",
    "        #print (hidden.view(1,-1).shape)\n",
    "        #print (hidden[2].shape)\n",
    "        return hidden[2]\n",
    "        #return hidden.view(-1,hidden[2].shape[1]*3)\n",
    "\n",
    "\n",
    "class FC_layer(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_size, output_size,p):\n",
    "        super(FC_layer, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, 1024)\n",
    "        #self.bn1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        #self.fc2 = nn.Linear(1024, 512)\n",
    "        #self.bn2 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc3 = nn.Linear(1024, output_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        fc_out = self.relu(self.dropout(self.fc1(hidden)))\n",
    "        #fc_out = self.relu(self.dropout(self.fc2(fc_out)))\n",
    "        #fc_out = self.dropout(self.fc2(fc_out))\n",
    "        fc_out = self.fc3(fc_out)\n",
    "        \n",
    "        return fc_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be the same for both RNN's\n",
    "hidden_size = 1024  \n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "\n",
    "input_size_encoder = len(en_vocab)\n",
    "en_embedding_size = 400\n",
    "en_dropout = 0.4\n",
    "\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, \n",
    "    en_embedding_size, \n",
    "    hidden_size, \n",
    "    num_layers, \n",
    "    en_dropout).to(device)\n",
    "\n",
    "de_dropout = 0.4\n",
    "\n",
    "output_size = len(set(y))\n",
    "\n",
    "FC_layer = FC_layer(hidden_size, \n",
    "                   output_size,\n",
    "                   de_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, FC_layer):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.FC_layer = FC_layer\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "\n",
    "        hidden = self.encoder(source)\n",
    "        outputs = self.FC_layer(hidden)\n",
    "        \n",
    "        #outputs = self.softmax(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Training hyperparameters\n",
    "#epochs = 10\n",
    "#learning_rate = 0.00005\n",
    "\n",
    "model = Seq2Seq(encoder_net, FC_layer).to(device)\n",
    "model.load_state_dict(torch.load(\"test_model.pth\"), strict=False)\n",
    "model.to(device)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched = False\n",
    "\n",
    "def validate(val_dl):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_dl.create_batches()\n",
    "        loop = tqdm.tqdm(enumerate(val_dl.batches), total=len(val_dl),leave=False)\n",
    "        for i,batch in loop:\n",
    "            batch_text = [example[\"text\"] for example in batch]\n",
    "            batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "            x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "            xvalc = x_padded.to(device)\n",
    "            yvalc = batch_label.to(device)\n",
    "            \n",
    "            '''loop = tqdm.tqdm(enumerate(val_dl), total=len(val_dl),leave=False)\n",
    "            for i, (xval,yval) in loop:\n",
    "            xval = xval.view(1,-1)\n",
    "            yval = yval.view(1,-1)\n",
    "            xvalc = xval.to(device)\n",
    "            yvalc = yval.to(device)'''\n",
    "\n",
    "            # Forward prop\n",
    "            output_val = model(xvalc.long(),yvalc)\n",
    "\n",
    "            #print (output_train.shape,ybc.shape)\n",
    "\n",
    "            accuracy.append(get_accuracy(output_val,yvalc))\n",
    "\n",
    "            loss_val = criterion(output_val, yvalc)\n",
    "            #loss_val = criterion(output_val, yvalc)\n",
    "\n",
    "            total_loss.append(loss_val.item())\n",
    "            \n",
    "            loop.set_postfix(loss = sum(total_loss)/(i+1),acc = sum(accuracy)/(len(accuracy)))\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)))\n",
    "    \n",
    "def train(train_dl):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    \n",
    "    train_dl.create_batches()\n",
    "    loop = tqdm.tqdm(enumerate(train_dl.batches), total=len(train_dl),leave=False)\n",
    "    \n",
    "    for i,batch in loop:\n",
    "        batch_text = [example[\"text\"] for example in batch]\n",
    "        batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "        \n",
    "        x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "        \n",
    "        xbc = x_padded.to(device)\n",
    "        ybc = batch_label.to(device)\n",
    "        \n",
    "        '''loop = tqdm.tqdm(enumerate(train_dl), total=len(train_dl),leave=False)\n",
    "        for i, (xb,yb) in loop:\n",
    "    \n",
    "        xb = xb.view(1,-1)\n",
    "        yb = yb.view(1,-1)\n",
    "        xbc = xb.to(device)\n",
    "        ybc = yb.to(device)'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward prop\n",
    "        output_train = model(xbc.long(),ybc)\n",
    "\n",
    "        accuracy.append(get_accuracy(output_train,ybc))\n",
    "        \n",
    "        \n",
    "        loss_train = criterion(output_train, ybc)\n",
    "        \n",
    "        # Back prop\n",
    "        loss_train.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss_train.item())\n",
    "        loop.set_postfix(loss = sum(total_loss)/(i+1),acc = sum(accuracy)/(len(accuracy)))\n",
    "        #if i % 1000 == 0:\n",
    "        #    print (\"Batch \" + str(i) + \" train loss = \" + str(sum(total_loss)/(i+1)) )\n",
    "\n",
    "        gc.collect()\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)))\n",
    "\n",
    "\n",
    "def get_accuracy(yhat,y): #  FOR BCE ERROR\n",
    "    \n",
    "    if batched:\n",
    "        batch_accuracy = []\n",
    "        for batch in range(yhat.shape[0]):\n",
    "            accuracy_list = []\n",
    "            \n",
    "            for i,entry in enumerate(yhat[batch]):\n",
    "                softmax = torch.exp(entry.float())\n",
    "                prob = list(softmax.cpu().detach().numpy())\n",
    "                predictions = np.argmax(prob, axis=0)\n",
    "                accuracy_list.append(np.argmax(y[batch][i].cpu().detach().numpy(), axis=0) == predictions != 0)\n",
    "            batch_accuracy.append((np.sum(np.array(accuracy_list))*1.0)/len(accuracy_list))\n",
    "            \n",
    "        return np.sum(batch_accuracy)/len(batch_accuracy)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        accuracy_list = []\n",
    "        #print (yhat.shape[0])\n",
    "        for i,entry in enumerate(yhat):\n",
    "            #if y[i].cpu().detach().numpy() != 0:\n",
    "                #print (entry.shape)\n",
    "            softmax = torch.exp(entry.float())\n",
    "            prob = list(softmax.cpu().detach().numpy())\n",
    "            predictions = np.argmax(prob, axis=0)\n",
    "                #if random.random() > 0.99:\n",
    "                #    print (predictions,y[i],y[i].cpu().detach().numpy())\n",
    "            #print (yhat,predictions,y[i].cpu().detach().numpy())\n",
    "            accuracy_list.append(y[i].cpu().detach().numpy() == predictions)\n",
    "            \n",
    "        return (np.sum(np.array(accuracy_list))*1.0)/len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq2Seq(\n",
       "   (encoder): Encoder(\n",
       "     (dropout): Dropout(p=0.4, inplace=False)\n",
       "     (embedding): Embedding(153, 400)\n",
       "     (rnn): LSTM(400, 1024, num_layers=3, batch_first=True, dropout=0.4)\n",
       "   )\n",
       "   (FC_layer): FC_layer(\n",
       "     (dropout): Dropout(p=0.4, inplace=False)\n",
       "     (relu): ReLU()\n",
       "     (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "     (fc3): Linear(in_features=1024, out_features=2, bias=True)\n",
       "   )\n",
       "   (softmax): Softmax(dim=None)\n",
       " ),\n",
       " odict_keys(['encoder.embedding.weight', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.weight_ih_l2', 'encoder.rnn.weight_hh_l2', 'encoder.rnn.bias_ih_l2', 'encoder.rnn.bias_hh_l2', 'FC_layer.fc1.weight', 'FC_layer.fc1.bias', 'FC_layer.fc3.weight', 'FC_layer.fc3.bias']))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = model.state_dict()\n",
    "model,params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Freezing layers except FC\n",
    "model.encoder.embedding.weight.requires_grad = False\n",
    "\n",
    "'''\n",
    "lstm_list = [model.encoder.rnn.parameters()]\n",
    "\n",
    "for entry in lstm_list:\n",
    "    for param in entry:\n",
    "        param.requires_grad = False'''\n",
    "\n",
    "model.encoder.rnn.bias_hh_l2.requires_grad = False\n",
    "model.encoder.rnn.bias_ih_l2.requires_grad = False\n",
    "model.encoder.rnn.weight_hh_l2.requires_grad = False\n",
    "model.encoder.rnn.weight_ih_l2.requires_grad = False\n",
    "\n",
    "model.encoder.rnn.bias_hh_l1.requires_grad = False\n",
    "model.encoder.rnn.bias_ih_l1.requires_grad = False\n",
    "model.encoder.rnn.weight_hh_l1.requires_grad = False\n",
    "model.encoder.rnn.weight_ih_l1.requires_grad = False\n",
    "\n",
    "model.encoder.rnn.bias_hh_l0.requires_grad = False\n",
    "model.encoder.rnn.bias_ih_l0.requires_grad = False\n",
    "model.encoder.rnn.weight_hh_l0.requires_grad = False\n",
    "model.encoder.rnn.weight_ih_l0.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# List to store values\n",
    "train_loss_list = []\n",
    "train_accu_list = []\n",
    "val_loss_list = []\n",
    "val_accu_list = []\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8312 [00:00<?, ?it/s, acc=1, loss=0.545]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.6800836638275882  val: 0.6434549726707078 \tACCU train: 0.5718238691049086  val: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<20:49,  6.65it/s, acc=0.5, loss=0.617]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.6703533400029403  val: 0.6425993777972795 \tACCU train: 0.5892685274302214  val: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<15:35,  8.89it/s, acc=0.5, loss=0.621]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.6632429046898765  val: 0.6426255851984024 \tACCU train: 0.5926371511068335  val: 0.6168831168831169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<14:13,  9.74it/s, acc=0, loss=0.726]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 \t LOSS train: 0.661892155759015  val: 0.6632232047069124 \tACCU train: 0.5993743984600578  val: 0.5887445887445888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<16:33,  8.36it/s, acc=0.5, loss=0.54]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 \t LOSS train: 0.6580739614873752  val: 0.63133594742069 \tACCU train: 0.6083974975938402  val: 0.6147186147186147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<19:01,  7.28it/s, acc=0.5, loss=0.552]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 \t LOSS train: 0.6516212100414981  val: 0.631040486035409 \tACCU train: 0.6144128970163619  val: 0.6558441558441559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<14:51,  9.32it/s, acc=1, loss=0.411]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 \t LOSS train: 0.6500752282141116  val: 0.64042002691593 \tACCU train: 0.621390760346487  val: 0.6536796536796536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<14:58,  9.25it/s, acc=0.5, loss=0.615]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 \t LOSS train: 0.6471180005233889  val: 0.6260141281661017 \tACCU train: 0.6195861405197305  val: 0.6536796536796536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<18:54,  7.33it/s, acc=0, loss=1.15]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 \t LOSS train: 0.6443565319040369  val: 0.6298969881810668 \tACCU train: 0.6240375360923965  val: 0.645021645021645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<19:29,  7.10it/s, acc=0.5, loss=1.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 \t LOSS train: 0.6422331262150416  val: 0.6274585517131405 \tACCU train: 0.6231953801732435  val: 0.6320346320346321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<16:11,  8.56it/s, acc=0, loss=1.31]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11 \t LOSS train: 0.641963382176266  val: 0.6279579007522368 \tACCU train: 0.6295717035611165  val: 0.6515151515151515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<16:15,  8.52it/s, acc=1, loss=0.443]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12 \t LOSS train: 0.6388286588887422  val: 0.6722635819991946 \tACCU train: 0.6281280076997112  val: 0.6147186147186147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<14:38,  9.46it/s, acc=0.5, loss=0.548]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13 \t LOSS train: 0.6392049661445807  val: 0.6349352331775607 \tACCU train: 0.6307747834456208  val: 0.6255411255411255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<15:31,  8.92it/s, acc=0.5, loss=0.677]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14 \t LOSS train: 0.634528724193444  val: 0.6190052467894245 \tACCU train: 0.641121270452358  val: 0.6428571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<17:06,  8.09it/s, acc=0.5, loss=0.525]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15 \t LOSS train: 0.6319471980177074  val: 0.6289886760634261 \tACCU train: 0.634744947064485  val: 0.6558441558441559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<21:04,  6.57it/s, acc=1, loss=0.621]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16 \t LOSS train: 0.6321019136913378  val: 0.639000575650822 \tACCU train: 0.6393166506256015  val: 0.6341991341991342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<16:20,  8.47it/s, acc=1, loss=0.372]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17 \t LOSS train: 0.6309334848763952  val: 0.6406116881024786 \tACCU train: 0.6345043310875842  val: 0.6320346320346321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<19:50,  6.98it/s, acc=0.5, loss=0.776]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18 \t LOSS train: 0.6310245388425453  val: 0.6183864741033805 \tACCU train: 0.6443695861405198  val: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<16:18,  8.49it/s, acc=1, loss=0.384]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19 \t LOSS train: 0.631761137762269  val: 0.6174176999113776 \tACCU train: 0.6366698748796921  val: 0.6536796536796536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20 \t LOSS train: 0.626258629711239  val: 0.6393925104757924 \tACCU train: 0.6428055822906641  val: 0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"model = \" + str(model) + \"\\n\")\n",
    "    log_file.write(\"\\nEmbedding layer and LSTM frozen\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers except FC and lstm last layer\n",
    "model.encoder.rnn.bias_hh_l2.requires_grad = True\n",
    "model.encoder.rnn.bias_ih_l2.requires_grad = True\n",
    "model.encoder.rnn.weight_hh_l2.requires_grad = True\n",
    "model.encoder.rnn.weight_ih_l2.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<17:23,  7.96it/s, acc=0, loss=0.88]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.6164995755595124  val: 0.6203622532226306 \tACCU train: 0.6559191530317613  val: 0.6493506493506493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<17:01,  8.13it/s, acc=0.5, loss=0.74]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.6067965594239092  val: 0.6183070709437003 \tACCU train: 0.6639797882579404  val: 0.658008658008658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.6052754262446124  val: 0.6133684550141876 \tACCU train: 0.668070259865255  val: 0.6601731601731602\n"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 3\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"\\nEmbedding layer and LSTM frozen (except last layer)\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers except FC and lstm last layer\n",
    "model.encoder.rnn.bias_hh_l1.requires_grad = True\n",
    "model.encoder.rnn.bias_ih_l1.requires_grad = True\n",
    "model.encoder.rnn.weight_hh_l1.requires_grad = True\n",
    "model.encoder.rnn.weight_ih_l1.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8312 [00:00<?, ?it/s, acc=0, loss=0.987]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.6025406177746377  val: 0.612895367304226 \tACCU train: 0.6615736284889316  val: 0.6688311688311688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<19:23,  7.15it/s, acc=0, loss=1.53]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.5967966183756511  val: 0.6083622692531837 \tACCU train: 0.6738450433108758  val: 0.6688311688311688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.5952752657302792  val: 0.609065079214898 \tACCU train: 0.6768527430221367  val: 0.6536796536796536\n"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 3\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"\\nEmbedding layer and LSTM frozen (except last two layers)\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers except FC and lstm last layer\n",
    "model.encoder.rnn.bias_hh_l0.requires_grad = True\n",
    "model.encoder.rnn.bias_ih_l0.requires_grad = True\n",
    "model.encoder.rnn.weight_hh_l0.requires_grad = True\n",
    "model.encoder.rnn.weight_ih_l0.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<19:09,  7.23it/s, acc=1, loss=0.125]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.6108786672774139  val: 0.5953454472672888 \tACCU train: 0.6678296438883542  val: 0.6796536796536796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8312 [00:00<21:40,  6.39it/s, acc=1, loss=0.582]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.5943883096600262  val: 0.599868852145228 \tACCU train: 0.674446583253128  val: 0.6688311688311688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8312 [00:00<16:30,  8.39it/s, acc=0.5, loss=0.698]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.5749551072847582  val: 0.5975804674677002 \tACCU train: 0.6921318575553417  val: 0.6774891774891775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 3491/8312 [07:11<11:29,  6.99it/s, acc=0.717, loss=0.55] "
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"\\nOnly embedding layer frozen\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_list, label='Training loss')\n",
    "plt.plot(val_loss_list, label='Validation loss')\n",
    "plt.legend()\n",
    "if not trial:\n",
    "    plt.savefig(log_file_name[:-4] + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched = False\n",
    "\n",
    "def validate(val_dl):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_dl.create_batches()\n",
    "        loop = tqdm.tqdm(enumerate(val_dl.batches), total=len(val_dl),leave=False)\n",
    "        for i,batch in loop:\n",
    "            batch_text = [example[\"text\"] for example in batch]\n",
    "            batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "            x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "            xvalc = x_padded.to(device)\n",
    "            yvalc = batch_label.to(device)\n",
    "            \n",
    "            '''loop = tqdm.tqdm(enumerate(val_dl), total=len(val_dl),leave=False)\n",
    "            for i, (xval,yval) in loop:\n",
    "            xval = xval.view(1,-1)\n",
    "            yval = yval.view(1,-1)\n",
    "            xvalc = xval.to(device)\n",
    "            yvalc = yval.to(device)'''\n",
    "\n",
    "            # Forward prop\n",
    "            output_val = model(xvalc.long(),yvalc)\n",
    "\n",
    "            #print (output_train.shape,ybc.shape)\n",
    "\n",
    "            accuracy.append(get_accuracy(output_val,yvalc))\n",
    "\n",
    "            loss_val = criterion(output_val, yvalc)\n",
    "            #loss_val = criterion(output_val, yvalc)\n",
    "\n",
    "            total_loss.append(loss_val.item())\n",
    "            \n",
    "            loop.set_postfix(loss = sum(total_loss)/(i+1),acc = sum(accuracy)/(len(accuracy)))\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)))\n",
    "    \n",
    "def train(train_dl):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    \n",
    "    train_dl.create_batches()\n",
    "    loop = tqdm.tqdm(enumerate(train_dl.batches), total=len(train_dl),leave=False)\n",
    "    \n",
    "    for i,batch in loop:\n",
    "        batch_text = [example[\"text\"] for example in batch]\n",
    "        batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "        \n",
    "        x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "        \n",
    "        xbc = x_padded.to(device)\n",
    "        ybc = batch_label.to(device)\n",
    "        \n",
    "        '''loop = tqdm.tqdm(enumerate(train_dl), total=len(train_dl),leave=False)\n",
    "        for i, (xb,yb) in loop:\n",
    "    \n",
    "        xb = xb.view(1,-1)\n",
    "        yb = yb.view(1,-1)\n",
    "        xbc = xb.to(device)\n",
    "        ybc = yb.to(device)'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward prop\n",
    "        output_train = model(xbc.long(),ybc)\n",
    "\n",
    "        accuracy.append(get_accuracy(output_train,ybc))\n",
    "        \n",
    "        \n",
    "        loss_train = criterion(output_train, ybc)\n",
    "        \n",
    "        # Back prop\n",
    "        loss_train.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss_train.item())\n",
    "        loop.set_postfix(loss = sum(total_loss)/(i+1),acc = sum(accuracy)/(len(accuracy)))\n",
    "        #if i % 1000 == 0:\n",
    "        #    print (\"Batch \" + str(i) + \" train loss = \" + str(sum(total_loss)/(i+1)) )\n",
    "\n",
    "        gc.collect()\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)))\n",
    "\n",
    "\n",
    "def get_accuracy(yhat,y): #  FOR BCE ERROR\n",
    "    \n",
    "    if batched:\n",
    "        batch_accuracy = []\n",
    "        \n",
    "        for batch in range(yhat.shape[0]):\n",
    "            accuracy_list = []\n",
    "            \n",
    "            for i,entry in enumerate(yhat[batch]):\n",
    "                softmax = torch.exp(entry.float())\n",
    "                prob = list(softmax.cpu().detach().numpy())\n",
    "                predictions = np.argmax(prob, axis=0)\n",
    "                accuracy_list.append(np.argmax(y[batch][i].cpu().detach().numpy(), axis=0) == predictions != 0)\n",
    "            batch_accuracy.append((np.sum(np.array(accuracy_list))*1.0)/len(accuracy_list))\n",
    "            \n",
    "        return np.sum(batch_accuracy)/len(batch_accuracy)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        accuracy_list = []\n",
    "        for i,entry in enumerate(yhat):\n",
    "            #if y[i].cpu().detach().numpy() != 0:\n",
    "                #print (entry.shape)\n",
    "            softmax = torch.exp(entry.float())\n",
    "            prob = list(softmax.cpu().detach().numpy())\n",
    "            predictions = np.argmax(prob, axis=0)\n",
    "                #if random.random() > 0.99:\n",
    "                #    print (predictions,y[i],y[i].cpu().detach().numpy())\n",
    "            accuracy_list.append(y[i].cpu().detach().numpy() == predictions)\n",
    "            \n",
    "        return (np.sum(np.array(accuracy_list))*1.0)/len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 1\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"model = \" + str(model) + \"\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_dl,show_predictions=False):\n",
    "    real_and_predictions = []\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        val_dl.create_batches()\n",
    "        loop = tqdm.tqdm(enumerate(val_dl.batches), total=len(val_dl),leave=False)\n",
    "        for i,batch in loop:\n",
    "            batch_text = [example[\"text\"] for example in batch]\n",
    "            batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "            x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "            #y_padded = pad_sequence(batch_label,batch_first=True, padding_value=0)\n",
    "            xvalc = x_padded.to(device)\n",
    "            yvalc = batch_label.to(device)\n",
    "            \n",
    "            '''loop = tqdm.tqdm(enumerate(val_dl), total=len(val_dl),leave=False)\n",
    "            for i, (xval,yval) in loop:\n",
    "            xval = xval.view(1,-1)\n",
    "            yval = yval.view(1,-1)\n",
    "            xvalc = xval.to(device)\n",
    "            yvalc = yval.to(device)'''\n",
    "\n",
    "            # Forward prop\n",
    "            output_val = model(xvalc.long(),yvalc)\n",
    "\n",
    "\n",
    "            accuracy.append(get_accuracy(output_val,yvalc))\n",
    "\n",
    "            loss_val = criterion(output_val, yvalc)\n",
    "            total_loss.append(loss_val.item())\n",
    "            \n",
    "            #print (one_hot_to_index(yvalc)[0],one_hot_to_index(output_val)[0])\n",
    "            if show_predictions:\n",
    "                if batched:\n",
    "                    compound_list_original = index_to_word(yval_reshapedc.cpu().detach().numpy())\n",
    "                    \n",
    "                    compound_list_predicted = []\n",
    "                    for i,entry in enumerate(output_val.view(len(yval_reshaped),-1,len(vocab))):\n",
    "                        softmax = torch.exp(entry.float())\n",
    "                        prob = list(softmax.cpu().detach().numpy())\n",
    "                        predictions = np.argmax(prob, axis=1)\n",
    "                        #print (predictions.shape,output_val.shape,len(compound_list_original[i]))\n",
    "                        pred = \"\"\n",
    "                        for i,entry in enumerate(predictions):\n",
    "                            pred += index_word[entry]\n",
    "                        compound_list_predicted.append(pred) \n",
    "                        \n",
    "                    for i in range(len(compound_list_original)):\n",
    "                        real_and_predictions.append((compound_list_original[i],compound_list_predicted[i]))\n",
    "                else:\n",
    "                    pred = \"\"\n",
    "                    real = \"\"\n",
    "                    softmax = torch.exp(output_val.float())\n",
    "                    prob = list(softmax.cpu().detach().numpy())\n",
    "                    predictions = np.argmax(prob, axis=1)\n",
    "                    for i,entry in enumerate(predictions):\n",
    "                        pred += en_index_word[entry]\n",
    "                        #print (i)\n",
    "                        real += en_index_word[yvalc.cpu().detach().numpy()[i]]\n",
    "                    real_and_predictions.append((real,pred))\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)),real_and_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "loss,accuracy,prediction_list = test(test_iterator,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.803559277827541, 0.7708333333333334)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.803559277827541, 0.7708333333333334)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
