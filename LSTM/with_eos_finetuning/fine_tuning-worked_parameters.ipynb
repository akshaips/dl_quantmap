{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "\n",
    "import string\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = True # setting False saves the output files else not saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output log file\n",
    "if not trial:\n",
    "    try:\n",
    "        os.system(\"mkdir output_files\")\n",
    "    except:\n",
    "        print (\"Folder output_files already present\")\n",
    "\n",
    "    present_files = glob.glob(\"output_files/log_output_*.txt\")\n",
    "    log_file_name = \"output_files/log_output_\" + str(len(present_files) + 1) + \".txt\"\n",
    "    log_file = open(log_file_name,\"w\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_augmentation = 0 # Data augmentation multiplier\n",
    "input_file = \"ML_input_5338.txt\" # Input data containing smiles and label\n",
    "train_percentage = 0.8 # Fraction to use for training (valida and test would be half of remaining data)\n",
    "Number_of_workers = 8 # Number of CPU threads to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rdkit warnings (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove rdkit warning\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_canonical_smiles(molecule):\n",
    "    try:\n",
    "        canonical_smile = Chem.MolToSmiles(Chem.MolFromSmiles(molecule))\n",
    "    except:\n",
    "        canonical_smile = False\n",
    "    return canonical_smile\n",
    "\n",
    "def get_cluster_count(y_count):\n",
    "    cluster_count = {}\n",
    "    for y in y_count:\n",
    "        if y not in cluster_count:\n",
    "            cluster_count[y] = 1\n",
    "        else:\n",
    "            cluster_count[y] +=1\n",
    "    return (cluster_count)\n",
    "\n",
    "def randomize_smiles(smiles,random_smiles=[],iteration=5):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        ans = list(range(m.GetNumAtoms()))\n",
    "        np.random.shuffle(ans)\n",
    "        nm = Chem.RenumberAtoms(m,ans)\n",
    "        out_smiles = (Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False))\n",
    "    except:\n",
    "        return (False)\n",
    "    \n",
    "    if out_smiles not in random_smiles:\n",
    "        return out_smiles\n",
    "    else:\n",
    "        iteration -= 1\n",
    "        if iteration > 0:\n",
    "            out_smiles = randomize_smiles(smiles,random_smiles,iteration)\n",
    "            return out_smiles\n",
    "        return (False)\n",
    "    \n",
    "def augment_smiles(count,iteration,smiles):\n",
    "    random_smiles = []\n",
    "    for i in range(count):\n",
    "        if smiles != None:\n",
    "            out_smiles = randomize_smiles(smiles,random_smiles,iteration=iteration)\n",
    "            if out_smiles:\n",
    "                random_smiles.append(out_smiles)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    return random_smiles\n",
    "\n",
    "def unpack_and_write_list(smiles,label,filename):\n",
    "    for entry in smiles:\n",
    "        if type(entry) == list:\n",
    "            unpack_and_write_list(entry,label,filename)\n",
    "        else:\n",
    "            filename.write(entry + \",\" + str(label) + \"\\n\")\n",
    "    \n",
    "def smiles_augmentation(df, N_rounds=1,iteration=5,data_set_type=\"train\"):\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(\"data\")\n",
    "        os.mkdir(\"data/classification\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    filename = \"data/classification/\" + str(data_set_type) + \"_aug_canonical_smiles.csv\"\n",
    "\n",
    "    aug_out = open(filename,\"w\")\n",
    "\n",
    "    aug_out.write(\"Smiles,Label\\n\")\n",
    "        \n",
    "    labels = []\n",
    "    for label in df.groupby('Label'):\n",
    "        labels.append(label[0])\n",
    "    \n",
    "    augmentation_list = []\n",
    "    if type(N_rounds) == list:\n",
    "        assert(len(N_rounds) == len(labels))\n",
    "        augmentation_list = N_rounds\n",
    "    else:\n",
    "        for i in range(len(labels)):\n",
    "            augmentation_list.append(N_rounds)\n",
    "        \n",
    "    for label,augmentation in zip(labels,augmentation_list):\n",
    "    \n",
    "        canonical_smiles = df[df['Label'] == label]['Smiles'].to_list()\n",
    "\n",
    "        p = Pool(Number_of_workers)\n",
    "        func = partial(augment_smiles, augmentation, iteration)\n",
    "        augmented_smiles = list(tqdm.tqdm(p.imap(func, canonical_smiles), total=len(canonical_smiles),leave=False))\n",
    "        p.close()\n",
    "    \n",
    "        print (\"Saving data for label = \" + str(label))\n",
    "\n",
    "        unpack_and_write_list(augmented_smiles,label,filename=aug_out)\n",
    "\n",
    "        unpack_and_write_list(canonical_smiles,label,filename=aug_out)\n",
    "        \n",
    "        print (\"Saved data for label = \" + str(label))\n",
    "        \n",
    "    aug_out.close()\n",
    "    \n",
    "    return (pd.read_csv(filename, header=0).sample(frac=1).reset_index(drop=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (4982, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC1=C(C(=O)N2CCCCC2=N1)CCN3CCC(CC3)C4=NOC5=C4C...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC1=C(C(=CC=C1)NC2=CC=CC=C2C(=O)O)C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1=CC(=C(C(=C1)F)F)CNC(=O)C2=CC=C(C=C2)S(=O)(=O)N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC(=O)C1=CC=C(C=C1)OC(=O)CCCCCN=C(N)N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1=C2C(=CS1)C(=O)NC3=CC=CC=C3N2C(=O)CN4CCN(CC4)C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles  Label\n",
       "0  CC1=C(C(=O)N2CCCCC2=N1)CCN3CCC(CC3)C4=NOC5=C4C...      3\n",
       "1                CC1=C(C(=CC=C1)NC2=CC=CC=C2C(=O)O)C      1\n",
       "2  C1=CC(=C(C(=C1)F)F)CNC(=O)C2=CC=C(C=C2)S(=O)(=O)N      1\n",
       "3            CCOC(=O)C1=CC=C(C=C1)OC(=O)CCCCCN=C(N)N      1\n",
       "4  CC1=C2C(=CS1)C(=O)NC3=CC=CC=C3N2C(=O)CN4CCN(CC4)C      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantmap_data = pd.read_csv(input_file,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True) #,header=None)\n",
    "print('Dataset:', quantmap_data.shape)\n",
    "quantmap_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smiles\n",
       "Label        \n",
       "0         714\n",
       "1        1887\n",
       "2        1894\n",
       "3         259\n",
       "4         228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantmap_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_df = quantmap_data.groupby('Label').count()\n",
    "label_count_list = []\n",
    "for entry in range(len(label_count_df)):\n",
    "    label_count_list.append(label_count_df.iloc[entry][0])\n",
    "\n",
    "augmentation_list = []\n",
    "max_value = max(label_count_list)\n",
    "for entry in label_count_list:\n",
    "    augmentation_list.append(int(max_value/entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 0\n",
      "Saved data for label = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 1\n",
      "Saved data for label = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 2\n",
      "Saved data for label = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 3\n",
      "Saved data for label = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data for label = 4\n",
      "Saved data for label = 4\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "augmentation_list = [entry*number_of_augmentation for entry in augmentation_list]\n",
    "iteration = 10000\n",
    "# Augmentation for training data\n",
    "quantmap_data = smiles_augmentation(quantmap_data,N_rounds=augmentation_list,iteration=iteration,data_set_type=\"all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smiles\n",
       "Label        \n",
       "0         714\n",
       "1        1887\n",
       "2        1894\n",
       "3         259\n",
       "4         228"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantmap_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label = \"None\" #\"fingerprint\"\n",
    "class MolTokenizer():\n",
    "    def __init__(self, lang = 'en'):\n",
    "        self.lang = lang\n",
    "        \n",
    "    def tokenizer(self, output_label=None,smiles=None):\n",
    "        tokens = atomwise_tokenizer(smiles)\n",
    "        tokens.insert(0, \"<SOS>\") \n",
    "        tokens.append(\"<EOS>\") \n",
    "        if output_label != \"fingerprint\":\n",
    "            return tokens\n",
    "        else:\n",
    "            try:\n",
    "                fingerprint = smiles_fingerprint(smiles,\"morgan\", radius=2,bits=1024)\n",
    "                return tokens,fingerprint\n",
    "            except:\n",
    "                return None,None\n",
    "        \n",
    "    def add_special_cases(self, toks):\n",
    "        pass\n",
    "\n",
    "tok = MolTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make vocabulary\n",
    "\n",
    "def unpack_vocab_list(vocab_list,vocab_unpacked):\n",
    "    for entry in vocab_list:\n",
    "        if type(entry) == list:\n",
    "            unpack_vocab_list(entry,vocab_unpacked)\n",
    "        else:\n",
    "            vocab_unpacked.append(entry)\n",
    "            \n",
    "    return (vocab_unpacked)\n",
    "            \n",
    "def make_vocabulary(input_list):\n",
    "    p = Pool(Number_of_workers)\n",
    "    func = partial(tok.tokenizer, None)\n",
    "    vocab_list = list(tqdm.tqdm(p.imap(func, input_list), total=len(input_list),leave=False))\n",
    "    p.close()\n",
    "    vocab_unpacked = []\n",
    "    return (list(set(unpack_vocab_list(vocab_list,vocab_unpacked))))\n",
    "\n",
    "def make_word_index(vocab):\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    for i,entry in enumerate(vocab):\n",
    "        word_index[entry] = i\n",
    "        index_word[i] = entry\n",
    "    return (word_index,index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vocab_file = True\n",
    "\n",
    "if use_vocab_file:\n",
    "    en_vocab = open(\"en_vocab_.txt\",\"r\").read().strip(\"[]\").replace(\"'\", \"\").replace(\" \", \"\").split(\",\")\n",
    "    en_word_index,en_index_word = make_word_index(en_vocab)\n",
    "    \n",
    "else:\n",
    "    all_smiles = quantmap_data[\"Smiles\"].to_list()\n",
    "\n",
    "    en_vocab = [\"<PAD>\",\"<UNK>\"]\n",
    "    en_vocab.extend(make_vocabulary(all_smiles))\n",
    "    del all_smiles\n",
    "\n",
    "    ###\n",
    "    en_word_index,en_index_word = make_word_index(en_vocab)\n",
    "\n",
    "    vocab_output = open(\"en_vocab_.txt\",\"w\")\n",
    "    vocab_output.write(str(en_vocab))\n",
    "    vocab_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/anaconda3/envs/molpmofit/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "/home/jovyan/anaconda3/envs/molpmofit/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Smiles\n",
       "Label        \n",
       "0         259\n",
       "1         228"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unbalanced data and shuffling\n",
    "balanced_data = quantmap_data[quantmap_data.Label != 0][quantmap_data.Label != 1][quantmap_data.Label != 2]\n",
    "balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
    "balanced_data[\"Label\"].replace({3: 0,4:1}, inplace=True)\n",
    "balanced_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_tokens(df):\n",
    "    \n",
    "    if output_label != \"fingerprint\": # Maintain the y from the data as label\n",
    "        labels = []\n",
    "        for label in df.groupby('Label'):\n",
    "            labels.append(label[0])\n",
    "            \n",
    "        x = []\n",
    "        y = []\n",
    "        p = Pool(Number_of_workers)\n",
    "        for label in labels:\n",
    "            smiles_list = df[df['Label'] == label]['Smiles'].to_list()\n",
    "            \n",
    "            \n",
    "            func = partial(tok.tokenizer, None)\n",
    "            tokens = list(tqdm.tqdm(p.imap(func, smiles_list), total=len(smiles_list),leave=False))\n",
    "            \n",
    "            #p = Pool(Number_of_workers)\n",
    "            #tokens = list(tqdm.tqdm(p.imap(tok.tokenizer, smiles_list), total=len(smiles_list),leave=False))\n",
    "            #p.close()\n",
    "            #if len(tokens) < 100:\n",
    "            for entry in tokens:\n",
    "                if  5 < len(entry) <= 150:\n",
    "                #break\n",
    "                    x.append(entry)\n",
    "                    y.append(label)\n",
    "            #x.extend(tokens)\n",
    "            #y.extend([label for i in range(len(tokens))])\n",
    "        p.close()\n",
    "            \n",
    "    else: # Return fingerprint as label for each object\n",
    "        x = []\n",
    "        y = []\n",
    "        smiles_list = df['Smiles'].to_list()\n",
    "        \n",
    "        func = partial(tok.tokenizer, output_label)\n",
    "        \n",
    "        for entry in smiles_list:\n",
    "            xout,yout = (func(entry))\n",
    "            if xout != None:\n",
    "                x.append(xout)\n",
    "                y.append([int(entry) for entry in yout])\n",
    "        \n",
    "        print (str(len(smiles_list)-len(x)) + \" incorrect smiles detected and deleted\"  )\n",
    "        \n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "x,y= convert_smiles_to_tokens(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenthwise_x = [len(entry) for entry in x if len(entry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVzElEQVR4nO3dcbDdZX3n8ffHAFqFFpDoZAM06EYrdVdgs8iunV2rS0uANbLbzoStwFJ3I1vSYmtnN9qdxY6dWeoqrqw0GZQItC6sU7BmNJUyyI5jV5SEpUiMlIipRLIQqwtY1mLgu3+cX8rp9ebec2+eX3LPyfs1c+ec3/N7nnOfhwQ+/J7n93tOqgpJklp6waHugCRp8hgukqTmDBdJUnOGiySpOcNFktTcEYe6AwfDCSecUMuWLTvU3ZCksbJ169bvVNXi+bQ9LMJl2bJlbNmy5VB3Q5LGSpK/mG9bp8UkSc0ZLpKk5gwXSVJzhoskqTnDRZLUnOEiSWrOcJEkNWe4SJKaM1wkSc0dFk/oazwsW/fZkevuvOq8Hnsi6UB55SJJas5wkSQ1Z7hIkpozXCRJzRkukqTmDBdJUnOGiySpOcNFktScD1Gqd3N5OLL1Z/qwpXRoeOUiSWqu13BJck6SB5PsSLJumvNJck13/v4kZ3TlJyW5K8n2JNuSXDHU5r1Jvp3kvu7n3D7HIEmau96mxZIsAq4FzgZ2Afck2VRVXxuqthJY3v28Hljfve4F3lVV9yY5Btia5I6hth+qqg/01XdJ0oHpc83lTGBHVT0MkOQWYBUwHC6rgJuqqoC7kxybZElV7QZ2A1TVU0m2A0untJVm5dqMdGj0OS22FHhk6HhXVzanOkmWAacDXx4qXttNo21Mctx0vzzJmiRbkmzZs2fPPIcgSZqPPsMl05TVXOokORq4FXhnVT3ZFa8HXgmcxuDq5oPT/fKquq6qVlTVisWLF8+x65KkA9FnuOwCTho6PhF4dNQ6SY5kECyfqKrb9lWoqseq6tmqeg74KIPpN0nSAtJnuNwDLE9ySpKjgNXApil1NgEXd3eNnQU8UVW7kwS4HtheVVcPN0iyZOjwAuCB/oYgSZqP3hb0q2pvkrXA7cAiYGNVbUtyWXd+A7AZOBfYATwNXNo1fwNwEfDVJPd1Ze+pqs3A+5OcxmD6bCfwjr7GIEman16f0O/CYPOUsg1D7wu4fJp2X2T69Riq6qLG3ZQkNeYT+pKk5gwXSVJzhoskqTnDRZLUnOEiSWrOcJEkNWe4SJKaM1wkSc0ZLpKk5gwXSVJzhoskqTnDRZLUnOEiSWrOcJEkNWe4SJKaM1wkSc0ZLpKk5gwXSVJzhoskqTnDRZLUnOEiSWrOcJEkNWe4SJKaM1wkSc0ZLpKk5gwXSVJzhoskqTnDRZLUnOEiSWrOcJEkNddruCQ5J8mDSXYkWTfN+SS5pjt/f5IzuvKTktyVZHuSbUmuGGpzfJI7kjzUvR7X5xgkSXPXW7gkWQRcC6wETgUuTHLqlGorgeXdzxpgfVe+F3hXVb0GOAu4fKjtOuDOqloO3NkdS5IWkD6vXM4EdlTVw1X1DHALsGpKnVXATTVwN3BskiVVtbuq7gWoqqeA7cDSoTY3du9vBN7a4xgkSfNwRI+fvRR4ZOh4F/D6EeosBXbvK0iyDDgd+HJX9PKq2g1QVbuTvGy6X55kDYOrIU4++eR5D+JwtGzdZ0eqt/Oq83ruiaRx1eeVS6Ypq7nUSXI0cCvwzqp6ci6/vKquq6oVVbVi8eLFc2kqSTpAfYbLLuCkoeMTgUdHrZPkSAbB8omqum2ozmNJlnR1lgCPN+63JOkA9Rku9wDLk5yS5ChgNbBpSp1NwMXdXWNnAU90U10Brge2V9XV07S5pHt/CfDp/oYgSZqP3tZcqmpvkrXA7cAiYGNVbUtyWXd+A7AZOBfYATwNXNo1fwNwEfDVJPd1Ze+pqs3AVcAnk7wd+Bbwi32NQZI0P30u6NOFweYpZRuG3hdw+TTtvsj06zFU1V8Cb27bU0lSSz6hL0lqznCRJDVnuEiSmjNcJEnNGS6SpOYMF0lSc4aLJKk5w0WS1JzhIklqznCRJDVnuEiSmut1bzEtLKN+CZj2zy9Sk0bjlYskqTnDRZLUnOEiSWpupHBJcn4Sg0iSNJJRA2M18FCS9yd5TZ8dkiSNv5HCpareBpwOfAP4eJIvJVmT5JheeydJGksjT3VV1ZPArcAtwBLgAuDeJL/aU98kSWNq1DWXtyT5FPB54EjgzKpaCbwO+M0e+ydJGkOjPkT5C8CHquoLw4VV9XSSX27fLUnSOBt1Wmz31GBJ8rsAVXVn815JksbaqOFy9jRlK1t2RJI0OWacFkvy74BfAV6Z5P6hU8cAf9pnxyRJ42u2NZf/Dvwx8J+BdUPlT1XVd3vrlXSQuamn1NZs4VJVtTPJ5VNPJDnegJEkTWeUK5fzga1AARk6V8AreuqXJGmMzRguVXV+93rKwemOJGkSzLagf8ZM56vq3rbdkSRNgtmmxT44w7kC3jRT4yTnAB8GFgEfq6qrppxPd/5c4GngX+8LrCQbGUzJPV5Vrx1q817g3wJ7uqL3VNXmWcahHrgILml/ZpsW+9n5fnCSRcC1DJ6R2QXck2RTVX1tqNpKYHn383pgffcKcAPwEeCmaT7+Q1X1gfn2TZLUr9mmxd5UVZ9P8i+mO19Vt83Q/ExgR1U93H3WLcAqYDhcVgE3VVUBdyc5NsmSqtpdVV9Ismwug5EkLQyzTYv9UwabVf7zac4VMFO4LAUeGTrexfNXJTPVWQrsnqVfa5NcDGwB3lVV35taIckaYA3AySefPMvHSZJamm1a7Mru9dJ5fHamKat51JlqPfC+rt77GKwL/cjmmVV1HXAdwIoVK2b7TElSQ6Nuuf/SJNckuTfJ1iQfTvLSWZrtAk4aOj4ReHQedf6Wqnqsqp6tqueAjzKYfpMkLSCjblx5C4O7s/4lg+339wD/Y5Y29wDLk5yS5CgGX5W8aUqdTcDFGTgLeKKqZpwSS7Jk6PAC4IERxyBJOkhG/T6X46vqfUPHv5PkrTM1qKq9SdYCtzO4FXljVW1Lcll3fgOwmcFtyDsY3Ir8N9NvSW4G3gickGQXcGVVXQ+8P8lpDKbFdgLvGHEMkqSDZNRwuSvJauCT3fEvALM+5NA9f7J5StmGofcF/Mi+Zd25C/dTftGIfZYkHSKz3Yr8FM/vKfYbwB90p14AfB+4stfeSZLG0mx3ix1zsDoiSZoco06LkeQ4Bk/Sv2hf2dSvPpYkCUYMlyT/BriCwa3C9wFnAV9ilr3FJEmHp1FvRb4C+IfAX3T7jZ3O8xtHSpL0t4waLj+oqh8AJHlhVX0deHV/3ZIkjbNR11x2JTkW+CPgjiTfY5Yn6SVJh6+RwqWqLujevjfJXcBPAJ/rrVeSpLE2l7vFzgB+hsFzL39aVc/01itJ0lgbdePK/wTcCLwUOAH4eJL/2GfHJEnja9QrlwuB04cW9a8C7gV+p6+OSZLG16h3i+1k6OFJ4IXAN5r3RpI0EWbbW+y/MVhj+WtgW5I7uuOzgS/23z1J0jiabVpsS/e6FfjUUPn/7KU3mpdl62bdoFqSDqrZNq68cd/77gu/XtUdPlhVP+yzY5Kk8TXq3mJvZHC32E4G2++flOQSN66UJE1n1LvFPgj8XFU9CJDkVcDNwD/oq2OSpPE16t1iR+4LFoCq+nPgyH66JEkad6NeuWxNcj3w+93xLzFY5Jck6UeMGi6XMfiu+19jsObyBeD3+uqUJGm8zRouSV4AbK2q1wJX998lSdK4m3XNpaqeA/4syckHoT+SpAkw6rTYEgZP6H8F+Kt9hVX1ll56JcCHIyWNr1HD5bd77YUkaaLMtrfYixgs5v9d4KvA9VW192B0TJI0vmZbc7kRWMEgWFYyeJhSkqQZzTYtdmpV/T2A7jmXr/TfJenwMeq62s6rzuu5J1Jbs125/M3mlE6HSZJGNduVy+uSPNm9D/Bj3XGAqqof77V3kqSxNNuW+4sOVkckSZNj1I0r5yXJOUkeTLIjybppzifJNd35+5OcMXRuY5LHkzwwpc3xSe5I8lD3elyfY5AkzV1v4ZJkEXAtg7vMTgUuTHLqlGorgeXdzxpg/dC5G4BzpvnodcCdVbUcuLM7liQtIH1euZwJ7Kiqh6vqGeAWYNWUOquAm2rgbuDYJEsAui8i++40n7uKwS3SdK9v7aPzkqT56zNclgKPDB3v6srmWmeql1fVboDu9WXTVUqyJsmWJFv27Nkzp45Lkg5Mn+GSacpqHnXmpaquq6oVVbVi8eLFLT5SkjSiPsNlF3DS0PGJwKPzqDPVY/umzrrXxw+wn5KkxvoMl3uA5UlOSXIUsBrYNKXOJuDi7q6xs4An9k15zWATcEn3/hLg0y07LUk6cL2FS/dE/1rgdmA78Mmq2pbksiSXddU2Aw8DO4CPAr+yr32Sm4EvAa9OsivJ27tTVwFnJ3kIOLs7liQtIKNuuT8vVbWZQYAMl20Yel8Mvj55urYX7qf8L4E3N+ymJKmxXsPlcOIGhJL0vF6f0JckHZ4MF0lSc4aLJKk5w0WS1JwL+gfZqAv/Gm/+Oetw55WLJKk5w0WS1JzhIklqznCRJDVnuEiSmjNcJEnNGS6SpOYMF0lSc4aLJKk5w0WS1JzhIklqznCRJDVnuEiSmjNcJEnNGS6SpOYMF0lSc4aLJKk5w0WS1JzhIklqznCRJDVnuEiSmjNcJEnNGS6SpOZ6DZck5yR5MMmOJOumOZ8k13Tn709yxmxtk7w3ybeT3Nf9nNvnGCRJc9dbuCRZBFwLrAROBS5McuqUaiuB5d3PGmD9iG0/VFWndT+b+xqDJGl+jujxs88EdlTVwwBJbgFWAV8bqrMKuKmqCrg7ybFJlgDLRmgraRrL1n12pHo7rzqv557ocNbntNhS4JGh411d2Sh1Zmu7tptG25jkuOl+eZI1SbYk2bJnz575jkGSNA99hkumKasR68zUdj3wSuA0YDfwwel+eVVdV1UrqmrF4sWLR+qwJKmNPqfFdgEnDR2fCDw6Yp2j9te2qh7bV5jko8Bn2nVZktRCn1cu9wDLk5yS5ChgNbBpSp1NwMXdXWNnAU9U1e6Z2nZrMvtcADzQ4xgkSfPQ25VLVe1Nsha4HVgEbKyqbUku685vADYD5wI7gKeBS2dq2330+5OcxmCabCfwjr7GAKMvjkqSntfntBjdbcKbp5RtGHpfwOWjtu3KL2rcTUlSYz6hL0lqznCRJDXX67SYpDZc+9O48cpFktSc4SJJas5wkSQ155qLdJhyg0v1ySsXSVJzhoskqTnDRZLUnOEiSWrOcJEkNWe4SJKaM1wkSc0ZLpKk5nyIUtKCNJfNOn3Qc+HxykWS1JzhIklqznCRJDVnuEiSmnNBX1ITk7TL8iSN5VDxykWS1JzhIklqznCRJDXnmoskjZlxeMDUKxdJUnOGiySpOcNFktScay6SZjSX+f1DZaE/l7LQ+9cHr1wkSc31Gi5JzknyYJIdSdZNcz5JrunO35/kjNnaJjk+yR1JHupej+tzDJKkuestXJIsAq4FVgKnAhcmOXVKtZXA8u5nDbB+hLbrgDurajlwZ3csSVpA+rxyORPYUVUPV9UzwC3Aqil1VgE31cDdwLFJlszSdhVwY/f+RuCtPY5BkjQPfS7oLwUeGTreBbx+hDpLZ2n78qraDVBVu5O8bLpfnmQNg6shgO8neXA+g1igTgC+c6g70aNJHx9M/hj3O7787kHuST+/+wTgO63H0sc/mwP4zBOAn5xv4z7DJdOU1Yh1Rmk7o6q6DrhuLm3GRZItVbXiUPejL5M+Ppj8MTq+8deNcdl82/c5LbYLOGno+ETg0RHrzNT2sW7qjO718YZ9liQ10Ge43AMsT3JKkqOA1cCmKXU2ARd3d42dBTzRTXnN1HYTcEn3/hLg0z2OQZI0D71Ni1XV3iRrgduBRcDGqtqW5LLu/AZgM3AusAN4Grh0prbdR18FfDLJ24FvAb/Y1xgWsImc7hsy6eODyR+j4xt/BzTGVM1pKUOSpFn5hL4kqTnDRZLUnOGygCU5KcldSbYn2Zbkiq58orbASbIoyf9O8pnueNLGd2ySP0zy9e7P8h9N0hiT/Hr39/OBJDcnedG4jy/JxiSPJ3lgqGy/Y0ry7m6rqgeT/Pyh6fXo9jO+/9L9Hb0/yaeSHDt0bs7jM1wWtr3Au6rqNcBZwOXdNjiTtgXOFcD2oeNJG9+Hgc9V1U8Br2Mw1okYY5KlwK8BK6rqtQxuwFnN+I/vBuCcKWXTjqn7d3I18NNdm9/rtrBayG7gR8d3B/Daqvr7wJ8D74b5j89wWcCqandV3du9f4rBf5SWMkFb4CQ5ETgP+NhQ8SSN78eBfwJcD1BVz1TV/2WCxsjgrtMfS3IE8GIGz6SN9fiq6gvAd6cU729Mq4Bbquqvq+qbDO5+PfNg9HO+phtfVf1JVe3tDu9m8HwhzHN8hsuYSLIMOB34MlO2wAGm3QJnTPxX4N8Dzw2VTdL4XgHsAT7eTf19LMlLmJAxVtW3gQ8weCxgN4Nn1f6ECRnfFPsb0/62sRpnvwz8cfd+XuMzXMZAkqOBW4F3VtWTh7o/rSQ5H3i8qrYe6r706AjgDGB9VZ0O/BXjN0W0X926wyrgFODvAC9J8rZD26uD7oC3q1pIkvwWgyn5T+wrmqbarOMzXBa4JEcyCJZPVNVtXfGkbIHzBuAtSXYy2Pn6TUn+gMkZHwz+L29XVX25O/5DBmEzKWP8Z8A3q2pPVf0QuA34x0zO+Ibtb0yjbHU1FpJcApwP/FI9/xDkvMZnuCxgScJgrn57VV09dGoitsCpqndX1Ynd5nirgc9X1duYkPEBVNX/AR5J8uqu6M3A15icMX4LOCvJi7u/r29msDY4KeMbtr8xbQJWJ3lhklMYfD/VVw5B/w5IknOA/wC8paqeHjo1v/FVlT8L9Af4GQaXn/cD93U/5wIvZXC3ykPd6/GHuq8NxvpG4DPd+4kaH3AasKX7c/wj4LhJGiPw28DXgQeA3wdeOO7jA25msIb0Qwb/5/72mcYE/BbwDeBBYOWh7v88x7eDwdrKvv/WbDiQ8bn9iySpOafFJEnNGS6SpOYMF0lSc4aLJKk5w0WS1JzhIjWW5Nkk93U7Bf9Zkt9IMuO/a0mWJflXB6uPUt8MF6m9/1dVp1XVTwNnM3g26cpZ2iwDDBdNDJ9zkRpL8v2qOnro+BXAPcAJwE8yeNDwJd3ptVX1v5LcDbwG+CaDHXc/NV29gzQE6YAZLlJjU8OlK/se8FPAU8BzVfWDJMuBm6tqRZI3Ar9ZVed39V88Xb2DOhDpABxxqDsgHSb27Sx7JPCRJKcBzwKv2k/9UetJC5LhIvWsmxZ7lsEuulcCjzH4RsoXAD/YT7NfH7GetCC5oC/1KMliYAPwkRrMQf8EsLuqngMuYvC1wDCYLjtmqOn+6kljwTUXqbEkzwJfZTC1tZfBwvzVVfVct35yK/A0cBfwq1V1dPe9PZ9jsOh/A/CZ6eod7LFI82W4SJKac1pMktSc4SJJas5wkSQ1Z7hIkpozXCRJzRkukqTmDBdJUnP/H3510nnWFPhXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(lenthwise_x, density=True, bins=30)  # density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token_to_index(molecule):\n",
    "    idxs = []\n",
    "    for ch in molecule:\n",
    "        if ch in en_word_index:\n",
    "            idxs.append(en_word_index[ch])\n",
    "        else:\n",
    "            idxs.append(en_word_index[\"<UNK>\"])\n",
    "    return torch.tensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "x_indexed_token = []\n",
    "loop = tqdm.tqdm(x, total=len(x),leave=False)\n",
    "for entry in loop:\n",
    "    x_indexed_token.append(convert_token_to_index(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_test_percentage = (1 - train_percentage)/2\n",
    "\n",
    "data_to_use = balanced_data\n",
    "# Ratios\n",
    "train_ratio = int (len(data_to_use) * train_percentage)\n",
    "valid_ratio = train_ratio + int(len(data_to_use)*valid_test_percentage)\n",
    "test_ratio = valid_ratio + int(len(data_to_use)*valid_test_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make index to split into train and val set\n",
    "def make_index(len_data,train_ratio,valid_ratio,test_ratio):\n",
    "    \n",
    "    index = np.random.permutation(len_data)\n",
    "    \n",
    "    # Train index and val index\n",
    "    return (index[:train_ratio],index[train_ratio:valid_ratio],index[valid_ratio:test_ratio])\n",
    "\n",
    "train_index ,valid_index,test_index = make_index(len(data_to_use),train_ratio,valid_ratio,test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders by padding the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded = pad_sequence(x_indexed_token,batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "train_sample = SubsetRandomSampler(train_index)\n",
    "valid_sample = SubsetRandomSampler(valid_index)\n",
    "test_sample = SubsetRandomSampler(test_index)\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x_padded,torch.tensor(y))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                        sampler=train_sample)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                        sampler=valid_sample)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                        sampler=test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data):\n",
    "    \n",
    "    index = np.random.permutation(len(data))\n",
    "    \n",
    "    output_shuffled = []\n",
    "    for i in index:\n",
    "        output_shuffled.append(data[i])\n",
    "        \n",
    "    return (output_shuffled)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = shuffle_data([(entry,y[i]) for i,entry in enumerate(x_indexed_token) if i in train_index])\n",
    "valid_data = shuffle_data([(entry,y[i]) for i,entry in enumerate(x_indexed_token) if i in valid_index])\n",
    "test_data = shuffle_data([(entry,y[i]) for i,entry in enumerate(x_indexed_token) if i in test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset class for loading data.\n",
    "      This is where the data parsing happens.\n",
    "      This class is built with reusability in mind.\n",
    "      Arguments:\n",
    "      path (:obj:`str`):\n",
    "      Path to the data partition.\n",
    "      \"\"\"\n",
    "    \n",
    "    def __init__(self, data_tuple):\n",
    "\n",
    "        # Check if path exists.\n",
    "        #if not os.path.isdir(path):\n",
    "          # Raise error if path is invalid.\n",
    "          #raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "    \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        # Since the labels are defined by folders with data we loop \n",
    "        # through each label.\n",
    "        '''for label  in ['pos', 'neg']:\n",
    "            sentiment_path = os.path.join(path, label)\n",
    "\n",
    "            # Get all files from path.\n",
    "            files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n",
    "            # Go through each file and read its content.\n",
    "            for file_name in tqdm(files_names, desc=f'{label} Files'):\n",
    "                file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "                # Read content.\n",
    "                content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "                # Fix any unicode issues.\n",
    "                content = fix_text(content)\n",
    "                # Save content.\n",
    "                self.texts.append(content)\n",
    "                # Save labels.\n",
    "                self.labels.append(label)'''\n",
    "        for entry in data_tuple:\n",
    "            self.texts.append(entry[0])\n",
    "            self.labels.append(entry[1])\n",
    "        # Number of examples.\n",
    "        self.n_examples = len(self.labels)\n",
    "        return\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"When used `len` return the number of examples.\"\"\"\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Given an index return an example from the position.\n",
    "        Arguments:\n",
    "          item (:obj:`int`):\n",
    "              Index position to pick an example to return.\n",
    "        Returns:\n",
    "          :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed \n",
    "          to a model.\n",
    "        \"\"\"\n",
    "        return {'text':self.texts[item], 'label':self.labels[item]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MakeDataset(train_data)\n",
    "valid_dataset = MakeDataset(valid_data)\n",
    "test_dataset = MakeDataset(test_data)\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_dataset,\n",
    "    sort = False,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x['text']),\n",
    "    batch_size = batch_size,\n",
    "    device = device)\n",
    "\n",
    "valid_iterator = data.BucketIterator(\n",
    "    valid_dataset,\n",
    "    sort = False,\n",
    "    shuffle=True,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x['text']),\n",
    "    batch_size = batch_size,\n",
    "    device = device)\n",
    "\n",
    "test_iterator = data.BucketIterator(\n",
    "    test_dataset,\n",
    "    sort = False,\n",
    "    shuffle=True,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x['text']),\n",
    "    batch_size = batch_size,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) #,padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p,batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N,seq_length) where N is batch size\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (N,seq_length, embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (N,seq_length, hidden_size)\n",
    "        \n",
    "        #cat = torch.cat(hidden[0],hidden[1],hidden[2],dim=2)\n",
    "        #print (hidden.view(1,-1).shape)\n",
    "        #print (hidden[2].shape)\n",
    "        return hidden[2]\n",
    "\n",
    "\n",
    "class FC_layer(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_size, output_size,p):\n",
    "        super(FC_layer, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        #self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(1024, output_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        fc_out = self.relu(self.dropout(self.fc1(hidden)))\n",
    "        #fc_out = self.dropout(self.fc2(fc_out))\n",
    "        fc_out = self.fc3(fc_out)\n",
    "        \n",
    "        return fc_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be the same for both RNN's\n",
    "hidden_size = 1024  \n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "\n",
    "input_size_encoder = len(en_vocab)\n",
    "en_embedding_size = 400\n",
    "en_dropout = 0.4\n",
    "\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, \n",
    "    en_embedding_size, \n",
    "    hidden_size, \n",
    "    num_layers, \n",
    "    en_dropout).to(device)\n",
    "\n",
    "de_dropout = 0.4\n",
    "\n",
    "output_size = len(set(y))\n",
    "\n",
    "FC_layer = FC_layer(hidden_size, \n",
    "                   output_size,\n",
    "                   de_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, FC_layer):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.FC_layer = FC_layer\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "\n",
    "        hidden = self.encoder(source)\n",
    "        outputs = self.FC_layer(hidden)\n",
    "        \n",
    "        #outputs = self.softmax(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Training hyperparameters\n",
    "#epochs = 10\n",
    "#learning_rate = 0.00005\n",
    "\n",
    "model = Seq2Seq(encoder_net, FC_layer).to(device)\n",
    "model.load_state_dict(torch.load(\"test_model.pth\"), strict=False)\n",
    "model.to(device)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched = False\n",
    "\n",
    "def validate(val_dl):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_dl.create_batches()\n",
    "        loop = tqdm.tqdm(enumerate(val_dl.batches), total=len(val_dl),leave=False)\n",
    "        for i,batch in loop:\n",
    "            batch_text = [example[\"text\"] for example in batch]\n",
    "            batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "            x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "            xvalc = x_padded.to(device)\n",
    "            yvalc = batch_label.to(device)\n",
    "            \n",
    "            '''loop = tqdm.tqdm(enumerate(val_dl), total=len(val_dl),leave=False)\n",
    "            for i, (xval,yval) in loop:\n",
    "            xval = xval.view(1,-1)\n",
    "            yval = yval.view(1,-1)\n",
    "            xvalc = xval.to(device)\n",
    "            yvalc = yval.to(device)'''\n",
    "\n",
    "            # Forward prop\n",
    "            output_val = model(xvalc.long(),yvalc)\n",
    "\n",
    "            #print (output_train.shape,ybc.shape)\n",
    "\n",
    "            accuracy.append(get_accuracy(output_val,yvalc))\n",
    "\n",
    "            loss_val = criterion(output_val, yvalc)\n",
    "            #loss_val = criterion(output_val, yvalc)\n",
    "\n",
    "            total_loss.append(loss_val.item())\n",
    "            \n",
    "            loop.set_postfix(loss = sum(total_loss)/(i+1),acc = sum(accuracy)/(len(accuracy)))\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)))\n",
    "    \n",
    "def train(train_dl):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    \n",
    "    train_dl.create_batches()\n",
    "    loop = tqdm.tqdm(enumerate(train_dl.batches), total=len(train_dl),leave=False)\n",
    "    \n",
    "    for i,batch in loop:\n",
    "        batch_text = [example[\"text\"] for example in batch]\n",
    "        batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "        \n",
    "        x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "        \n",
    "        xbc = x_padded.to(device)\n",
    "        ybc = batch_label.to(device)\n",
    "        \n",
    "        '''loop = tqdm.tqdm(enumerate(train_dl), total=len(train_dl),leave=False)\n",
    "        for i, (xb,yb) in loop:\n",
    "    \n",
    "        xb = xb.view(1,-1)\n",
    "        yb = yb.view(1,-1)\n",
    "        xbc = xb.to(device)\n",
    "        ybc = yb.to(device)'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward prop\n",
    "        output_train = model(xbc.long(),ybc)\n",
    "\n",
    "        accuracy.append(get_accuracy(output_train,ybc))\n",
    "        \n",
    "        \n",
    "        loss_train = criterion(output_train, ybc)\n",
    "        \n",
    "        # Back prop\n",
    "        loss_train.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss_train.item())\n",
    "        loop.set_postfix(loss = sum(total_loss)/(i+1),acc = sum(accuracy)/(len(accuracy)))\n",
    "        #if i % 1000 == 0:\n",
    "        #    print (\"Batch \" + str(i) + \" train loss = \" + str(sum(total_loss)/(i+1)) )\n",
    "\n",
    "        gc.collect()\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)))\n",
    "\n",
    "\n",
    "def get_accuracy(yhat,y): #  FOR BCE ERROR\n",
    "    \n",
    "    if batched:\n",
    "        batch_accuracy = []\n",
    "        for batch in range(yhat.shape[0]):\n",
    "            accuracy_list = []\n",
    "            \n",
    "            for i,entry in enumerate(yhat[batch]):\n",
    "                softmax = torch.exp(entry.float())\n",
    "                prob = list(softmax.cpu().detach().numpy())\n",
    "                predictions = np.argmax(prob, axis=0)\n",
    "                accuracy_list.append(np.argmax(y[batch][i].cpu().detach().numpy(), axis=0) == predictions != 0)\n",
    "            batch_accuracy.append((np.sum(np.array(accuracy_list))*1.0)/len(accuracy_list))\n",
    "            \n",
    "        return np.sum(batch_accuracy)/len(batch_accuracy)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        accuracy_list = []\n",
    "        #print (yhat.shape[0])\n",
    "        for i,entry in enumerate(yhat):\n",
    "            #if y[i].cpu().detach().numpy() != 0:\n",
    "                #print (entry.shape)\n",
    "            softmax = torch.exp(entry.float())\n",
    "            prob = list(softmax.cpu().detach().numpy())\n",
    "            predictions = np.argmax(prob, axis=0)\n",
    "                #if random.random() > 0.99:\n",
    "                #    print (predictions,y[i],y[i].cpu().detach().numpy())\n",
    "            #print (yhat,predictions,y[i].cpu().detach().numpy())\n",
    "            accuracy_list.append(y[i].cpu().detach().numpy() == predictions)\n",
    "            \n",
    "        return (np.sum(np.array(accuracy_list))*1.0)/len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq2Seq(\n",
       "   (encoder): Encoder(\n",
       "     (dropout): Dropout(p=0.4, inplace=False)\n",
       "     (embedding): Embedding(153, 400)\n",
       "     (rnn): LSTM(400, 1024, num_layers=3, batch_first=True, dropout=0.4)\n",
       "   )\n",
       "   (FC_layer): FC_layer(\n",
       "     (dropout): Dropout(p=0.4, inplace=False)\n",
       "     (relu): ReLU()\n",
       "     (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "     (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (fc3): Linear(in_features=1024, out_features=2, bias=True)\n",
       "   )\n",
       "   (softmax): Softmax(dim=None)\n",
       " ),\n",
       " odict_keys(['encoder.embedding.weight', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.weight_ih_l2', 'encoder.rnn.weight_hh_l2', 'encoder.rnn.bias_ih_l2', 'encoder.rnn.bias_hh_l2', 'FC_layer.fc1.weight', 'FC_layer.fc1.bias', 'FC_layer.bn1.weight', 'FC_layer.bn1.bias', 'FC_layer.bn1.running_mean', 'FC_layer.bn1.running_var', 'FC_layer.bn1.num_batches_tracked', 'FC_layer.fc3.weight', 'FC_layer.fc3.bias']))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = model.state_dict()\n",
    "model,params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Freezing layers except FC\n",
    "model.encoder.embedding.weight.requires_grad = False\n",
    "\n",
    "'''\n",
    "lstm_list = [model.encoder.rnn.parameters()]\n",
    "\n",
    "for entry in lstm_list:\n",
    "    for param in entry:\n",
    "        param.requires_grad = False'''\n",
    "\n",
    "model.encoder.rnn.bias_hh_l2.requires_grad = False\n",
    "model.encoder.rnn.bias_ih_l2.requires_grad = False\n",
    "model.encoder.rnn.weight_hh_l2.requires_grad = False\n",
    "model.encoder.rnn.weight_ih_l2.requires_grad = False\n",
    "\n",
    "model.encoder.rnn.bias_hh_l1.requires_grad = False\n",
    "model.encoder.rnn.bias_ih_l1.requires_grad = False\n",
    "model.encoder.rnn.weight_hh_l1.requires_grad = False\n",
    "model.encoder.rnn.weight_ih_l1.requires_grad = False\n",
    "\n",
    "model.encoder.rnn.bias_hh_l0.requires_grad = False\n",
    "model.encoder.rnn.bias_ih_l0.requires_grad = False\n",
    "model.encoder.rnn.weight_hh_l0.requires_grad = False\n",
    "model.encoder.rnn.weight_ih_l0.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# List to store values\n",
    "train_loss_list = []\n",
    "train_accu_list = []\n",
    "val_loss_list = []\n",
    "val_accu_list = []\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:41,  9.32it/s, acc=0.5, loss=0.71]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.6923312346140543  val: 0.6690205360452334 \tACCU train: 0.5193798449612403  val: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:36, 10.65it/s, acc=0.5, loss=0.709]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.6669940711299887  val: 0.6468556560575962 \tACCU train: 0.5917312661498708  val: 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/387 [00:00<00:36, 10.65it/s, acc=0.333, loss=0.648]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.6498014102148455  val: 0.6248916697998842 \tACCU train: 0.6201550387596899  val: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:37, 10.28it/s, acc=0.5, loss=0.702]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 \t LOSS train: 0.6349860369697098  val: 0.604510085657239 \tACCU train: 0.6330749354005168  val: 0.7083333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:48,  7.92it/s, acc=1, loss=0.585]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 \t LOSS train: 0.618421283418202  val: 0.5864958564440409 \tACCU train: 0.6718346253229974  val: 0.7916666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:30, 12.58it/s, acc=1, loss=0.5]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 \t LOSS train: 0.6079784158896414  val: 0.5739298947155476 \tACCU train: 0.710594315245478  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:34, 11.28it/s, acc=1, loss=0.59]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 \t LOSS train: 0.6008615667043731  val: 0.556405900667111 \tACCU train: 0.6744186046511628  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:34, 11.21it/s, acc=1, loss=0.576]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 \t LOSS train: 0.5888994351529951  val: 0.5446737806002299 \tACCU train: 0.7131782945736435  val: 0.7291666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:28, 13.46it/s, acc=1, loss=0.423]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 \t LOSS train: 0.5864000950364795  val: 0.5361069142818451 \tACCU train: 0.7028423772609819  val: 0.7291666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:27, 13.79it/s, acc=1, loss=0.603]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 \t LOSS train: 0.5778208503581449  val: 0.5269977239271005 \tACCU train: 0.7183462532299741  val: 0.7291666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:41,  9.30it/s, acc=0.5, loss=0.426]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11 \t LOSS train: 0.5692662722996655  val: 0.5182087700814009 \tACCU train: 0.7286821705426356  val: 0.7291666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:28, 13.35it/s, acc=0.667, loss=0.614]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12 \t LOSS train: 0.5581377026184585  val: 0.50816081960996 \tACCU train: 0.7286821705426356  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:29, 13.27it/s, acc=1, loss=0.251]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13 \t LOSS train: 0.552869386990249  val: 0.503656471769015 \tACCU train: 0.7416020671834626  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:35, 10.88it/s, acc=1, loss=0.496]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14 \t LOSS train: 0.5505163166720122  val: 0.49209347429374856 \tACCU train: 0.7286821705426356  val: 0.7708333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:34, 11.21it/s, acc=0.5, loss=0.774]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15 \t LOSS train: 0.5444499570731969  val: 0.49075148813426495 \tACCU train: 0.7674418604651163  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:41,  9.28it/s, acc=0.5, loss=1.57]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16 \t LOSS train: 0.5358444466147312  val: 0.48337651789188385 \tACCU train: 0.7777777777777778  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:28, 13.71it/s, acc=1, loss=0.29]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17 \t LOSS train: 0.5375331715711944  val: 0.47886891787250835 \tACCU train: 0.7441860465116279  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:33, 11.38it/s, acc=0.667, loss=0.365]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18 \t LOSS train: 0.5352387538405968  val: 0.47208687166372937 \tACCU train: 0.7312661498708011  val: 0.7708333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:45,  8.55it/s, acc=0, loss=0.804]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19 \t LOSS train: 0.5320383514390743  val: 0.4679629386713107 \tACCU train: 0.7596899224806202  val: 0.7708333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20 \t LOSS train: 0.5227129468622134  val: 0.4712115420649449 \tACCU train: 0.7416020671834626  val: 0.7708333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 20\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"model = \" + str(model) + \"\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:54,  7.11it/s, acc=1, loss=0.0728]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.5291801138903743  val: 0.46264240394035977 \tACCU train: 0.7209302325581395  val: 0.7708333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:36, 10.60it/s, acc=0.5, loss=0.6]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.5180818517848811  val: 0.4581955422957738 \tACCU train: 0.7571059431524548  val: 0.7708333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:49,  7.72it/s, acc=1, loss=0.453]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.5201664612428778  val: 0.4545918752749761 \tACCU train: 0.7648578811369509  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:48,  7.99it/s, acc=0.5, loss=0.519]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 \t LOSS train: 0.5192020501585278  val: 0.4521295993278424 \tACCU train: 0.7674418604651163  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 \t LOSS train: 0.5167312874658472  val: 0.4501076533148686 \tACCU train: 0.7596899224806202  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers except FC and lstm last layer\n",
    "model.encoder.rnn.bias_hh_l2.requires_grad = True\n",
    "model.encoder.rnn.bias_ih_l2.requires_grad = True\n",
    "model.encoder.rnn.weight_hh_l2.requires_grad = True\n",
    "model.encoder.rnn.weight_ih_l2.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<01:07,  5.69it/s, acc=1, loss=0.313]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.5058865799768335  val: 0.44978309484819573 \tACCU train: 0.7596899224806202  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:46,  8.37it/s, acc=1, loss=0.354]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.5096207077805078  val: 0.4496996756643057 \tACCU train: 0.772609819121447  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.5138090574772167  val: 0.4496207336584727 \tACCU train: 0.772609819121447  val: 0.75\n"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 3\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"model = \" + str(model) + \"\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "    torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers except FC and lstm last layer\n",
    "model.encoder.rnn.bias_hh_l1.requires_grad = True\n",
    "model.encoder.rnn.bias_ih_l1.requires_grad = True\n",
    "model.encoder.rnn.weight_hh_l1.requires_grad = True\n",
    "model.encoder.rnn.weight_ih_l1.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:50,  7.58it/s, acc=1, loss=0.363]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.5077965658456473  val: 0.4495837123443683 \tACCU train: 0.7493540051679587  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:37, 10.17it/s, acc=1, loss=0.376]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.5107460513287428  val: 0.4495598083982865 \tACCU train: 0.7545219638242894  val: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.5060303726399592  val: 0.44952931565543014 \tACCU train: 0.7777777777777778  val: 0.75\n"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 3\n",
    "learning_rate = 1e-7\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"model = \" + str(model) + \"\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "    torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers except FC and lstm last layer\n",
    "model.encoder.rnn.bias_hh_l0.requires_grad = True\n",
    "model.encoder.rnn.bias_ih_l0.requires_grad = True\n",
    "model.encoder.rnn.weight_hh_l0.requires_grad = True\n",
    "model.encoder.rnn.weight_ih_l0.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:48,  7.93it/s, acc=0.5, loss=0.622]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t LOSS train: 0.5206841951192811  val: 0.40238638656834763 \tACCU train: 0.7545219638242894  val: 0.7916666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:57,  6.67it/s, acc=1, loss=0.374]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 \t LOSS train: 0.4630697392678076  val: 0.3593431667735179 \tACCU train: 0.7829457364341085  val: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:48,  7.99it/s, acc=1, loss=0.02]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 \t LOSS train: 0.4473544961414288  val: 0.33271334630747634 \tACCU train: 0.7803617571059431  val: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:38,  9.90it/s, acc=1, loss=0.17]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 \t LOSS train: 0.40563000865684923  val: 0.34969270912309486 \tACCU train: 0.8217054263565892  val: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:34, 11.13it/s, acc=1, loss=0.108]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 \t LOSS train: 0.3747086386347926  val: 0.3050452557702859 \tACCU train: 0.8217054263565892  val: 0.8541666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:40,  9.55it/s, acc=1, loss=0.149]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 \t LOSS train: 0.351103096584325  val: 0.28473584664364654 \tACCU train: 0.8320413436692506  val: 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:49,  7.70it/s, acc=0.5, loss=0.82]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 \t LOSS train: 0.3260248762384558  val: 0.4013729343811671 \tACCU train: 0.8656330749354005  val: 0.8541666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:42,  9.07it/s, acc=1, loss=0.0635]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 \t LOSS train: 0.30124654218515995  val: 0.3180166694025199 \tACCU train: 0.8656330749354005  val: 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/387 [00:00<00:41,  9.31it/s, acc=1, loss=0.0382]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 \t LOSS train: 0.27914783135249016  val: 0.2930549525966247 \tACCU train: 0.8811369509043928  val: 0.8958333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/387 [00:00<00:45,  8.45it/s, acc=1, loss=0.0921]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 \t LOSS train: 0.23576106566175317  val: 0.30006720560292405 \tACCU train: 0.9018087855297158  val: 0.8958333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 261/387 [00:29<00:15,  8.13it/s, acc=0.923, loss=0.216]"
     ]
    }
   ],
   "source": [
    "#loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"model = \" + str(model) + \"\\n\")\n",
    "    log_file.write(\"learning_rate = \" + str(learning_rate) + \"\\n\")\n",
    "    log_file.write(\"epochs = \" + str(epochs) + \"\\n\")\n",
    "    log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print (\"EPOCH \" + str(epoch))\n",
    "    \n",
    "    train_loss, train_accu = train(train_iterator)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accu_list.append(train_accu)\n",
    "    \n",
    "    val_loss,val_accu = validate(valid_iterator)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accu_list.append(val_accu)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "    torch.save(model.state_dict(), \"pretrained_model_epoch\" + str(epoch) + \".pth\")\n",
    "    print (\"Epoch :\",epoch+1,\"\\t\",\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_dl,show_predictions=False):\n",
    "    real_and_predictions = []\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        val_dl.create_batches()\n",
    "        loop = tqdm.tqdm(enumerate(val_dl.batches), total=len(val_dl),leave=False)\n",
    "        for i,batch in loop:\n",
    "            batch_text = [example[\"text\"] for example in batch]\n",
    "            batch_label = torch.tensor([example[\"label\"] for example in batch])\n",
    "            x_padded = pad_sequence(batch_text,batch_first=True, padding_value=0)\n",
    "            #y_padded = pad_sequence(batch_label,batch_first=True, padding_value=0)\n",
    "            xvalc = x_padded.to(device)\n",
    "            yvalc = batch_label.to(device)\n",
    "            \n",
    "            '''loop = tqdm.tqdm(enumerate(val_dl), total=len(val_dl),leave=False)\n",
    "            for i, (xval,yval) in loop:\n",
    "            xval = xval.view(1,-1)\n",
    "            yval = yval.view(1,-1)\n",
    "            xvalc = xval.to(device)\n",
    "            yvalc = yval.to(device)'''\n",
    "\n",
    "            # Forward prop\n",
    "            output_val = model(xvalc.long(),yvalc)\n",
    "\n",
    "\n",
    "            accuracy.append(get_accuracy(output_val,yvalc))\n",
    "\n",
    "            loss_val = criterion(output_val, yvalc)\n",
    "            total_loss.append(loss_val.item())\n",
    "            \n",
    "            #print (one_hot_to_index(yvalc)[0],one_hot_to_index(output_val)[0])\n",
    "            if show_predictions:\n",
    "                if batched:\n",
    "                    compound_list_original = index_to_word(yval_reshapedc.cpu().detach().numpy())\n",
    "                    \n",
    "                    compound_list_predicted = []\n",
    "                    for i,entry in enumerate(output_val.view(len(yval_reshaped),-1,len(vocab))):\n",
    "                        softmax = torch.exp(entry.float())\n",
    "                        prob = list(softmax.cpu().detach().numpy())\n",
    "                        predictions = np.argmax(prob, axis=1)\n",
    "                        #print (predictions.shape,output_val.shape,len(compound_list_original[i]))\n",
    "                        pred = \"\"\n",
    "                        for i,entry in enumerate(predictions):\n",
    "                            pred += index_word[entry]\n",
    "                        compound_list_predicted.append(pred) \n",
    "                        \n",
    "                    for i in range(len(compound_list_original)):\n",
    "                        real_and_predictions.append((compound_list_original[i],compound_list_predicted[i]))\n",
    "                else:\n",
    "                    pred = \"\"\n",
    "                    real = \"\"\n",
    "                    softmax = torch.exp(output_val.float())\n",
    "                    prob = list(softmax.cpu().detach().numpy())\n",
    "                    predictions = np.argmax(prob, axis=1)\n",
    "                    for i,entry in enumerate(predictions):\n",
    "                        pred += en_index_word[entry]\n",
    "                        #print (i)\n",
    "                        real += en_index_word[yvalc.cpu().detach().numpy()[i]]\n",
    "                    real_and_predictions.append((real,pred))\n",
    "    return (sum(total_loss)/(i+1),sum(accuracy)/(len(accuracy)),real_and_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "loss,accuracy,prediction_list = test(test_iterator,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5378021039068699, 0.7708333333333334)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
