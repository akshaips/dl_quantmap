{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "import math\n",
    "import tqdm\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(file_object,chunk_size=20):\n",
    "    while True:\n",
    "        dm_file = file_object.readlines(chunk_size)\n",
    "        if not dm_file:\n",
    "            break\n",
    "        yield dm_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i,entry in enumerate(chunk):\\n        entry = entry.strip()\\n        if entry.strip() != \"\":\\n            data = json.loads(entry)\\n            if i == 0:\\n                df = pd.DataFrame.from_dict(data)\\n            else:\\n                df1= pd.DataFrame.from_dict(data)\\n                df = pd.concat([df,df1], axis=1)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i,entry in enumerate(chunk):\n",
    "        entry = entry.strip()\n",
    "        if entry.strip() != \"\":\n",
    "            data = json.loads(entry)\n",
    "            if i == 0:\n",
    "                df = pd.DataFrame.from_dict(data)\n",
    "            else:\n",
    "                df1= pd.DataFrame.from_dict(data)\n",
    "                df = pd.concat([df,df1], axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_as_df(chunk):\n",
    "    json_data = [json.loads(line) for line in chunk]\n",
    "    df = pd.concat([pd.DataFrame.from_dict(entry) for entry in json_data], axis=1)\n",
    "    df.index = row_names\n",
    "    return (df,row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = \"1000_compounds.json\"\n",
    "row_names = list(map(int,open(input_filename[:-5] + \"_cidorder.txt\",\"r\").read().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_dist = 0.01\n",
    "cids = row_names\n",
    "\n",
    "clusters = {}\n",
    "cluster_leaf_dist = {}\n",
    "check_list = set()\n",
    "cluster_output = {}\n",
    "file_object = open(input_filename,\"r\")\n",
    "chunked_data = read_chunk(file_object,chunk_size=1014100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "#for i in range(number_of_batches):\n",
    "for n,chunk in enumerate(chunked_data):    \n",
    "    batch_size = len(chunk)\n",
    "    beginning = n * batch_size\n",
    "    end = (n+1) * batch_size\n",
    "    distancd_df,small_list = get_as_df(chunk)\n",
    "    \n",
    "    number_of_batches = len(row_names)//batch_size\n",
    "    \n",
    "    df = distancd_df\n",
    "    \n",
    "    total_data_points = len(df)\n",
    "    \n",
    "    \n",
    "    # For each datapoint (i.e., all the cids) find the distance with the cid in batch \n",
    "    # Then choose the one with distance less than the min_distance given\n",
    "    # Then use that to make a dict of clusters of format\n",
    "    # {all_cid1 : [batch_cids1],all_cid2: [batch_cids2]}\n",
    "    # batch_cids1 and batch_cids2 etc are cids with length less than min distance\n",
    "    # And also cluster_leaf_dist with format\n",
    "    # {all_cid1 : [batch_cids1_distance],all_cid2: [batch_cids2_distance]}\n",
    "    \n",
    "    loop = tqdm.tqdm(range(total_data_points), total=total_data_points,leave=False)\n",
    "    \n",
    "    for j in loop:\n",
    "        do_cluster = True\n",
    "        \n",
    "        if cids[j]  in check_list:\n",
    "            do_cluster = False\n",
    "            \n",
    "        if do_cluster:\n",
    "            df1 = df.iloc[j][df.iloc[j] < min_dist]\n",
    "            cluster_leaf_dist[cids[j]] = list(df1)\n",
    "            clusters[cids[j]] = list(map(int,df1.index))\n",
    "            \n",
    "            if len(check_list) != (batch_size * (n + 1)):\n",
    "                a = list(clusters.values())[-1]\n",
    "                check_list.update(a)\n",
    "                \n",
    "        loop.set_description(\"Batch \" + str(n+1) + \"/\" + str(number_of_batches))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find number of occurance of each label in different clusters\n",
    "    # For each {all_cid1 : [batch_cids1],all_cid2: [batch_cids2]}\n",
    "    # {batch_cids1_1 : occurance,batch_cids1_2:occurance ...}\n",
    "    \n",
    "    label_dict = {entry : 0 for entry in cids[0:end]}\n",
    "    for label_list in list(clusters.values()):\n",
    "        for entry in label_list:\n",
    "            label_dict[entry] += 1\n",
    "    \n",
    "    # For each {batch_cids1_1 : occurance,batch_cids1_2:occurance ...}\n",
    "    # if occurance > 1\n",
    "    # batch_cids1_1\n",
    "    loop = tqdm.tqdm(label_dict, total=len(label_dict),leave=False)      \n",
    "    # Find the one with more than one occurance and remove from the clusters\n",
    "    for leaf in loop:\n",
    "        if label_dict[leaf] > 1:\n",
    "            leaf_index = small_list.index(leaf)\n",
    "            root_leaf_distance = {}\n",
    "            root_list = []\n",
    "            reached_limit = False\n",
    "            \n",
    "            root_leaf_same = False\n",
    "            # Find the distance for the leaf in each root/cluster centers\n",
    "            i = 0\n",
    "            for root in clusters:\n",
    "                if len(root_leaf_distance) == label_dict[leaf]:\n",
    "                    reached_limit = True\n",
    "                    break\n",
    "                if not reached_limit:\n",
    "                    if leaf in clusters[root]:\n",
    "                        root_list.append(root)\n",
    "                        root_leaf_distance[root] = cluster_leaf_dist[root][clusters[root].index(leaf)] + (0.0000001*i) #m.iloc[root_index,leaf_index]\n",
    "                        if root == leaf:\n",
    "                            root_leaf_distance[root] = cluster_leaf_dist[root][clusters[root].index(leaf)]\n",
    "                            root_leaf_same = True\n",
    "                            root_leaf_cid = root\n",
    "                        i += 1\n",
    "                                \n",
    "            assert (len(root_leaf_distance) == label_dict[leaf])\n",
    "            \n",
    "            root_leaf_distance_values = list(root_leaf_distance.values())\n",
    "            root_leaf_distance_keys = list(root_leaf_distance.keys())\n",
    "            \n",
    "            if not root_leaf_same:\n",
    "                # Find the root with least distance\n",
    "                min_value_index = root_leaf_distance_values.index(min(root_leaf_distance_values))\n",
    "                belongs_to_root = root_list[min_value_index]\n",
    "\n",
    "                # Remove the element from the cluster for the non-root\n",
    "                root_list.remove(belongs_to_root)\n",
    "            \n",
    "            else:\n",
    "                # Sort distance to find the second least distance if leaf == root in the above loop\n",
    "                # This is to avoid self falling into the cluster with len 0\n",
    "                sorted_distance = sorted(root_leaf_distance_values)\n",
    "                second_minimum = sorted_distance[1]\n",
    "                second_min_distance = second_minimum\n",
    "                if second_min_distance <= min_dist:\n",
    "                    min_value_index = root_leaf_distance_values.index(second_min_distance)\n",
    "                    belongs_to_root = root_list[min_value_index]\n",
    "                else:\n",
    "                    min_value_index = root_leaf_distance_values.index(min(root_leaf_distance_values))\n",
    "                    belongs_to_root = root_list[min_value_index]\n",
    "                root_list.remove(belongs_to_root)\n",
    "                \n",
    "            for entry in root_list:\n",
    "                clusters[entry].remove(leaf)\n",
    "        loop.set_description(\"Finding and sorting the data to clusters Batch \" + str(n+1) + \"/\" + str(number_of_batches))\n",
    "\n",
    "    for entry in clusters:\n",
    "        if entry not in cluster_output:\n",
    "            cluster_output[entry] = []\n",
    "            cluster_output[entry].extend(clusters[entry])\n",
    "        else:\n",
    "            for entry1 in clusters[entry]:\n",
    "                cluster_output[entry].append(entry1)\n",
    "                cluster_output[entry] = list(set(cluster_output[entry]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(modified_dict,key,output_list=[],finished_key = []):\n",
    "    output_list.append(key)\n",
    "    finished_key.append(key)\n",
    "    \n",
    "    try:\n",
    "        value_of_current_key = modified_dict[key]\n",
    "    except:\n",
    "        value_of_current_key = []\n",
    "    \n",
    "    for entry in value_of_current_key:\n",
    "        try:\n",
    "            output_list.extend(modified_dict[entry])\n",
    "        except:\n",
    "            pass\n",
    "        output_list.append(entry)\n",
    "        finished_key.append(entry)\n",
    "    \n",
    "    for entry in modified_dict:\n",
    "        if key in modified_dict[entry]:\n",
    "            output_list.extend(modified_dict[entry])\n",
    "            output_list.append(entry)\n",
    "    \n",
    "    return (list(set(output_list)),finished_key)\n",
    "\n",
    "def get_all_values(modified_dict,key,output_list=[],finished_key = []):\n",
    "    input_list = []\n",
    "    input_list.append(key)\n",
    "    \n",
    "    for entry in modified_dict:\n",
    "        if key in modified_dict[entry]:\n",
    "            input_list.extend(modified_dict[entry])\n",
    "            input_list.append(entry)\n",
    "    \n",
    "    input_list = list(set(input_list))\n",
    "    \n",
    "    for entry in input_list:\n",
    "        if entry not in finished_key:\n",
    "            output_list,finished_key = get_values(modified_dict,entry,output_list,finished_key)\n",
    "    \n",
    "    for key in output_list:\n",
    "        if key not in finished_key:\n",
    "            output_list,finished_key = get_values(modified_dict,key,output_list,finished_key)\n",
    "            \n",
    "    return output_list,finished_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dict =  copy.deepcopy(cluster_output)\n",
    "final_output = {}\n",
    "clusterd_cids = []\n",
    "#print (clusterd_cids,modified_dict,final_output)\n",
    "for entry in modified_dict:\n",
    "    current_values = []\n",
    "    finished_key = []\n",
    "    \n",
    "    if entry not in clusterd_cids:\n",
    "        #print (entry,clusterd_cids)\n",
    "        current_values,finished_key = get_all_values(modified_dict,entry,[],[])\n",
    "        final_output[entry] = list(set(current_values))\n",
    "        clusterd_cids.extend(list(set(current_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_output)#,final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "check_list = []\n",
    "i = 0\n",
    "for entry in final_output:\n",
    "    if len(final_output[entry]) > 0:\n",
    "        cluster_dict[i] = final_output[entry]\n",
    "        for entry1 in final_output[entry]:\n",
    "            check_list.append(entry1)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 567)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(check_list)),len(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_leader = {}\n",
    "check_list = []\n",
    "for entry in cluster_dict:\n",
    "    #if len(cluster_dict[entry]) > 0:\n",
    "    for entry1 in cluster_dict[entry]:\n",
    "        cluster_leader[entry1] = entry\n",
    "        check_list.append(entry1)\n",
    "    #else:\n",
    "    #    check_list.append(entry)\n",
    "    #    cluster_leader[entry] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 567, 1000)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(check_list)),len(cluster_dict),len(cluster_leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "cid_output_file = open(input_filename[:-5] + \"qmap_clustering.txt\",\"r\").readlines()\n",
    "cluster_actual = {}\n",
    "for entry in cid_output_file:\n",
    "    cluster_actual[int(entry.split()[0])] = int(entry.split()[1])\n",
    "print (len(cluster_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n"
     ]
    }
   ],
   "source": [
    "print (len(set(cluster_actual.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comaprison of K-mean and K-mean distance matrix\n",
    "# Given two dict, it compares the overlap (dict of cid and cluster)\n",
    "# Perfect overlap will have each label having list of one high number and rest all zero\n",
    "def find_cluster_overlap(dict1,dict2):\n",
    "    \n",
    "    dict_1_clusters = {int(dict1[entry]) : [] for entry in dict1.keys()}\n",
    "    for entry in dict1.keys():\n",
    "        dict_1_clusters[int(dict1[entry])].append(int(entry))\n",
    "    \n",
    "    dict_2_clusters = {int(dict2[entry]) : [] for entry in dict2.keys()}\n",
    "    for entry in dict2.keys():\n",
    "        dict_2_clusters[int(dict2[entry])].append(int(entry))\n",
    "    \n",
    "    all_label_clustered = {int(dict1[entry]) : [0 for entry in dict_2_clusters.keys()] for entry in dict1.keys()}\n",
    "\n",
    "    for label1 in dict_1_clusters:\n",
    "        for label2 in dict_2_clusters:\n",
    "            for entry in dict_1_clusters[label1]:\n",
    "                if entry in dict_2_clusters[label2]:\n",
    "                    all_label_clustered[label1][label2] += 1\n",
    "\n",
    "    total = 0\n",
    "    for entry in all_label_clustered:\n",
    "        total += sum(all_label_clustered[entry])\n",
    "    return (total,all_label_clustered)\n",
    "total_clusters,label_overlap = find_cluster_overlap(cluster_leader,cluster_actual)\n",
    "#18 {4: [5, 1, 0, 0, 1], 0: [0, 4, 0, 0, 1], 3: [0, 0, 0, 1, 0], 2: [0, 0, 0, 0, 2], 1: [0, 0, 3, 0, 0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_clusters\n",
    "#for entry in label_overlap:\n",
    "#    print (entry,[i for i,entry in enumerate(label_overlap[entry]) if entry > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [4, 49, 5, 2, 4, 2, 2, 1, 1]\n",
      "127 [1, 2]\n",
      "156 [1, 1]\n",
      "174 [2, 3]\n",
      "220 [1, 1]\n",
      "524 [3, 1]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for entry in label_overlap:\n",
    "    multi_list = []\n",
    "    for entry1 in label_overlap[entry]:\n",
    "        if entry1 >= 1:\n",
    "            multi_list.append(entry1)\n",
    "    if len(multi_list) > 1:\n",
    "        print (entry,multi_list)\n",
    "        count += 1\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_max(label_overlap,actual_label,chosen_max,index):\n",
    "    for entry in label_overlap:\n",
    "        if entry!= actual_label:\n",
    "            current_max = label_overlap[entry][index]\n",
    "            if current_max > chosen_max:\n",
    "                found_new_max = True\n",
    "                return found_new_max\n",
    "    return False\n",
    "\n",
    "def get_accuracy(label_overlap):\n",
    "    \n",
    "    finished_clusters = []\n",
    "    each_row_accuracy = {}\n",
    "    for actual_label in label_overlap:\n",
    "        count_dict = {} # Count of clusters more than 0 and their index in the list for each label in the actual dict\n",
    "        for i,count in enumerate(label_overlap[actual_label]):\n",
    "            if count > 0:\n",
    "                count_dict[i] = count\n",
    "        \n",
    "        # Find the clusters which is mostly in the label, then find whether it is accumulated maximum in the label, if yes find accuracy else\n",
    "        # find the next max and do the procedure\n",
    "        calculated_accuracy = False\n",
    "        count_dict = {k: v for k, v in sorted(count_dict.items(), key=lambda item: item[1])}\n",
    "        for i in count_dict:\n",
    "            if i not in finished_clusters:\n",
    "                chosen_max = count_dict[i]\n",
    "                if not get_other_max(label_overlap,actual_label,chosen_max,i):\n",
    "                    row_accuracy = chosen_max/(sum(count_dict.values()))\n",
    "                    calculated_accuracy = True\n",
    "                    finished_clusters.append(i)\n",
    "                    break\n",
    "        \n",
    "        # If accuracy was never found with the max element from the above code, use the max value in the list to get the accuracy\n",
    "        if not calculated_accuracy:\n",
    "            for i in count_dict:\n",
    "                if i not in finished_clusters:\n",
    "                    chosen_max = count_dict[i]\n",
    "                    row_accuracy = chosen_max/(sum(count_dict.values()))\n",
    "                    calculated_accuracy = True\n",
    "                    finished_clusters.append(i)\n",
    "                    break\n",
    "        \n",
    "        if not calculated_accuracy:\n",
    "            row_accuracy = 0\n",
    "        \n",
    "        each_row_accuracy[actual_label] = row_accuracy\n",
    "    row_accuracy = each_row_accuracy.values()\n",
    "    print (sum(row_accuracy)/len(row_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985063408079281\n"
     ]
    }
   ],
   "source": [
    "get_accuracy(label_overlap)\n",
    "#0.9454352646321534\n",
    "#0.9301186220463726\n",
    "#0.9296067787115542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-292-d4656f0701e5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-292-d4656f0701e5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    5757: 4,\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "5757: 4,\n",
    "448537: 4,\n",
    "2733525: 4,\n",
    "104741: 4, \n",
    "6623: 4,\n",
    "1752: 4,\n",
    "5281576: 4,\n",
    "    \n",
    "5035: 0,\n",
    "5280961: 0,\n",
    "5281707: 0,\n",
    "3224: 0,\n",
    "    \n",
    "445154: 5,\n",
    "\n",
    "3026: 3,\n",
    "\n",
    "3476: 2,\n",
    "77999: 2,\n",
    "    \n",
    "2244: 1,\n",
    "3672: 1,\n",
    "3032: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
