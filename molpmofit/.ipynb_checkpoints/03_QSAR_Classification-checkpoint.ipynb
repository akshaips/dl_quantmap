{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QSAR/QSPR Models Fine-Tuning 1: Classification \n",
    "\n",
    "This notebook is an example of a classification task on BBBP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import sqlite3\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "from SmilesPE.spe2vec import Corpus\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from utils import *\n",
    "import torch\n",
    "print (torch.__version__)\n",
    "\n",
    "import supp_utils as su\n",
    "\n",
    "torch.cuda.set_device(0) #change to 0 if you only has one GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('results')\n",
    "name = 'classification_new'\n",
    "path = data_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove rdkit and other warning\n",
    "\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = True # setting False saves the output files else not saved\n",
    "\n",
    "multi_files = False\n",
    "\n",
    "if multi_files:\n",
    "    input_file = [\"first_5000.txt\",\"ML_input_5338.txt\"]\n",
    "else:\n",
    "    input_file = \"data/ML_input_5338_out.txt\" # Input data containing smiles and label\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"Used files \" + str(input_file) + \"\\n\")\n",
    "    \n",
    "number_of_augmentation = 1 # Data augmentation multiplier\n",
    "train_percentage = 0.7 # Fraction to use for training (validation and test would be half of remaining data)\n",
    "Number_of_workers = 8 # Number of CPU threads to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data with lower distribution\n",
    "lower_label_count_cutoff = 1000\n",
    "upper_label_count_cutoff = 5000\n",
    "enable_label_cutoff = True\n",
    "\n",
    "if not multi_files and enable_label_cutoff:\n",
    "    open_file = open(input_file,\"r\").readlines()\n",
    "    \n",
    "    # Find the  number of labels (count of each label)\n",
    "    label_count_init = {}\n",
    "    for entry in open_file:\n",
    "        label = int(entry.split()[1])\n",
    "        if label in label_count_init:\n",
    "            label_count_init[label] += 1\n",
    "        else:\n",
    "            label_count_init[label] = 1\n",
    "    \n",
    "    # Select the label above the cutoff  (count of each label above cutoff)\n",
    "    label_count = {}\n",
    "    for entry in label_count_init:\n",
    "        if label_count_init[entry] > lower_label_count_cutoff and label_count_init[entry] < upper_label_count_cutoff:\n",
    "            label_count[entry] = label_count_init[entry]\n",
    "    \n",
    "    # Select only the smiles with labels above the cutoff\n",
    "    smiles_label = {}\n",
    "    allocated_label = []\n",
    "    allowed_labels = sorted(label_count.keys())\n",
    "    for entry in open_file:\n",
    "        smiles = entry.split()[0]\n",
    "        label = int(entry.split()[1])\n",
    "        if label in allowed_labels:\n",
    "            if label not in allocated_label:\n",
    "                allocated_label.append(label)\n",
    "            smiles_label[smiles] = allocated_label.index(label)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 1894, 1: 1887}\n",
      "\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print (label_count)\n",
    "print ()\n",
    "print (allowed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dict ={}\n",
    "for entry in smiles_label:\n",
    "    if smiles_label[entry] not in check_dict:\n",
    "        check_dict[smiles_label[entry]] = 1\n",
    "    else:\n",
    "        check_dict[smiles_label[entry]] += 1\n",
    "#check_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (3781, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN(C)CCC1=CNC2=C1C=C(C=C2)CC3COC(=O)N3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C(C1C(C(C(C(O1)OC2C(OC(C(C2O)O)OC3C(OC(C(C3O)O...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1CCNC(C1)C(C2=CC(=NC3=C2C=CC=C3C(F)(F)F)C(F)(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN1C=NC2=C1C(=O)N=C(N2)N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1=C(N=CN1)CSCCNC(=NC)NC#N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Smiles  Label\n",
       "0             CN(C)CCC1=CNC2=C1C=C(C=C2)CC3COC(=O)N3      0\n",
       "1  C(C1C(C(C(C(O1)OC2C(OC(C(C2O)O)OC3C(OC(C(C3O)O...      1\n",
       "2  C1CCNC(C1)C(C2=CC(=NC3=C2C=CC=C3C(F)(F)F)C(F)(...      0\n",
       "3                           CN1C=NC2=C1C(=O)N=C(N2)N      1\n",
       "4                        CC1=C(N=CN1)CSCCNC(=NC)NC#N      0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_files:\n",
    "    for i,input_filename in enumerate(input_file):\n",
    "        if i != 0:\n",
    "            quantmap_data2 = pd.read_csv(input_filename,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True) #,header=None)\n",
    "            quantmap_data = pd.concat([quantmap_data,quantmap_data2])\n",
    "        else:\n",
    "            quantmap_data = pd.read_csv(input_filename,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True) #,header=None)\n",
    "    del quantmap_data2\n",
    "else:\n",
    "    quantmap_data = pd.read_csv(input_file,sep=\" \",names=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "if enable_label_cutoff:\n",
    "    quantmap_data = pd.DataFrame(smiles_label.items(),columns=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "print('Dataset:', quantmap_data.shape)\n",
    "quantmap_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Smiles\n",
      "Label        \n",
      "0        1894\n",
      "1        1887\n"
     ]
    }
   ],
   "source": [
    "print (quantmap_data.groupby('Label').count())\n",
    "if not trial:\n",
    "    log_file.write(\"Class distribution before augmentation\\n\")\n",
    "    log_file.write(str(quantmap_data.groupby('Label').count()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unbalanced data and shuffling\n",
    "balance_data = False\n",
    "if balance_data:\n",
    "    balanced_data = quantmap_data[quantmap_data.Label != 0][quantmap_data.Label != 3][quantmap_data.Label != 4]\n",
    "    balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
    "    balanced_data[\"Label\"].replace({1: 0,2:1}, inplace=True)\n",
    "    quantmap_data = balanced_data\n",
    "    balanced_data.groupby('Label').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \r"
     ]
    }
   ],
   "source": [
    "quantmap_data = su.sanity_check(quantmap_data,None,Number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_test_percentage = (1 - train_percentage)/2\n",
    "\n",
    "data_to_use = quantmap_data\n",
    "# Ratios\n",
    "train_ratio = int (len(data_to_use) * train_percentage)\n",
    "valid_ratio = train_ratio + int(len(data_to_use)*valid_test_percentage)\n",
    "test_ratio = valid_ratio + int(len(data_to_use)*valid_test_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make index to split into train and val set\n",
    "np.random.seed(3)\n",
    "def make_index(len_data,train_ratio,valid_ratio,test_ratio):\n",
    "    \n",
    "    index = np.random.permutation(len_data)\n",
    "    \n",
    "    # Train index and val index\n",
    "    return (index[:train_ratio],index[train_ratio:valid_ratio],index[valid_ratio:test_ratio])\n",
    "\n",
    "train_index ,valid_index,test_index = make_index(len(data_to_use),train_ratio,valid_ratio,test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = quantmap_data[quantmap_data.index.isin(train_index)]\n",
    "valid_data = quantmap_data[quantmap_data.index.isin(valid_index)]\n",
    "test_data = quantmap_data[quantmap_data.index.isin(test_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.9985], device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count_list = np.array([entry for entry in train_data.groupby('Label').count()[\"Smiles\"]])\n",
    "class_weight = np.min(class_count_list)/class_count_list\n",
    "class_weight = torch.FloatTensor(class_weight).cuda()\n",
    "\n",
    "if not trial:\n",
    "    log_file.write(\"Class weight for loss (balancing weights)= \" + str(class_weight) + \"\\n\")\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation_list = su.get_augmentation_list(train_data,number_of_augmentation)\n",
    "valid_augmentation_list = su.get_augmentation_list(valid_data,number_of_augmentation)\n",
    "test_augmentation_list = su.get_augmentation_list(test_data,number_of_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \r"
     ]
    }
   ],
   "source": [
    "###\n",
    "iteration = 1000000\n",
    "# Augmentation for training data\n",
    "train_aug = su.smiles_augmentation(train_data,N_rounds=train_augmentation_list,iteration=iteration,data_set_type=\"train_data\",Number_of_workers=Number_of_workers)\n",
    "valid_aug = su.smiles_augmentation(valid_data,N_rounds=valid_augmentation_list,iteration=iteration,data_set_type=\"valid_data\",Number_of_workers=Number_of_workers)\n",
    "test_aug = su.smiles_augmentation(test_data,N_rounds=test_augmentation_list,iteration=iteration,data_set_type=\"test_data\",Number_of_workers=Number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Smiles\n",
      "Label        \n",
      "0        1322\n",
      "1        1324\n",
      "       Smiles\n",
      "Label        \n",
      "0         292\n",
      "1         275\n",
      "       Smiles\n",
      "Label        \n",
      "0         280\n",
      "1         287\n"
     ]
    }
   ],
   "source": [
    "print (train_data.groupby('Label').count())\n",
    "print (valid_data.groupby('Label').count())\n",
    "print (test_data.groupby('Label').count())\n",
    "if not trial:\n",
    "    log_file.write(\"number of augmentation = \" + str(number_of_augmentation) + \"\\n\")\n",
    "    log_file.write(\"Class distribution after augmentation\\n\")\n",
    "    log_file.write(\"Train data\\n\")\n",
    "    log_file.write(str(train_data.groupby('Label').count()) + \"\\n\")\n",
    "    log_file.write(\"Valid data\\n\")\n",
    "    log_file.write(str(valid_data.groupby('Label').count()) + \"\\n\")\n",
    "    log_file.write(\"Test data\\n\")\n",
    "    log_file.write(str(test_data.groupby('Label').count()) + \"\\n\")\n",
    "    log_file.write(\"Train/valid split ratio = \" + str(train_percentage) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adpot the Encoder of MSPM According to the Target Dataset.\n",
    "\n",
    "In order to fine-tuning the pre-trained MSPM on the QSAR datasets of interest, we need to prepare the data:\n",
    "\n",
    "- Tokenize the SMILES of the QSAR dataset.\n",
    "- Align the token IDs of the QSAR dataset to the token IDs pre-trained MSPM. \n",
    "\n",
    "Often, the vocab size of the QSAR dataset is different from that of the pre-trained strcuture prediction model, which means the QSAR model will have a different input size from that of pre-trained model. Here, we need to change the input size of the pre-trained model to the vocab size of the QSAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang = 'en'):\n",
    "        self.lang = lang\n",
    "        \n",
    "    def tokenizer(self, smiles):        \n",
    "        smiles = \"[BOS]\" + smiles\n",
    "        tokens = atomwise_tokenizer(smiles)\n",
    "        return tokens\n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "tok = Tokenizer(partial(MolTokenizer), n_cpus=8, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 96\n"
     ]
    }
   ],
   "source": [
    "qsar_vocab = TextLMDataBunch.from_df(path, train_aug, valid_aug, bs=bs, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols=0,label_cols=1, max_vocab=60000, include_bos=False)\n",
    "\n",
    "print(f'Vocab Size: {len(qsar_vocab.vocab.itos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = Path('results/pretraining_new/models')\n",
    "\n",
    "pretrained_fnames = ['pretraining_new_wt', 'pretraining_new_vocab']\n",
    "fnames = [pretrained_model_path/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_learner = language_model_learner(qsar_vocab, AWD_LSTM, drop_mult=1.0)\n",
    "lm_learner = lm_learner.load_pretrained(*fnames)\n",
    "lm_learner.freeze()\n",
    "lm_learner.save_encoder(f'lm_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databunch for QSAR Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to change the `text_cols` and `label_col` based on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas = TextClasDataBunch.from_df(path, train_aug, valid_aug, bs=bs, tokenizer=tok, \n",
    "                                          chunksize=50000, text_cols='Smiles',label_cols='Label', \n",
    "                                          vocab=qsar_vocab.vocab, max_vocab=60000, include_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[BOS] C C 1 C ( C ( C C ( O 1 ) O C 2 C ( C ( C ( O C 2 O C 3 = C 4 C = C 5 C = C 3 O C 6 = C ( C = C ( C = C 6 ) C ( C ( C ( = O ) N C ( C ( =</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[BOS] C C C ( C ) C ( C ( = O ) N C ( C C ( = O ) N ) C ( = O ) N C ( C C ( C ) C ) C ( = O ) N C ( C C C C N ) C ( = O ) N C ( C ) C ( = O ) N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[BOS] C ( C C C 1 C ( C ) ( C C ( N ) = O ) C 2 = C ( C ) C 3 = N C ( C ) ( C 4 C ( C C ( N ) = O ) C ( C ) ( C C C ( N C C ( O P ( = O ) ( O )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[BOS] C C 1 C ( C ( C C ( O 1 ) O C 2 C C ( C C 3 = C ( C 4 = C ( C ( = C 2 3 ) O ) C ( = O ) C 5 = C ( C 4 = O ) C = C C = C 5 O C ) O ) ( C (</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[BOS] C 1 ( C C ( C ) C ) N ( C ) C ( = O ) C N ( C ) C ( = O ) C ( C C ) N C ( = O ) C ( C ( C ( C ) C C = C C ) O ) N ( C ) C ( = O ) C ( C (</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUROC(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n",
    "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, train, **kwargs):\n",
    "        if not train:\n",
    "            self.output.append(last_output)\n",
    "            self.target.append(last_target)\n",
    "                \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if len(self.output) > 0:\n",
    "            output = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            metric = auroc_score(preds, target)\n",
    "            return add_metrics(last_metrics, [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learner = text_classifier_learner(data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "cls_learner.load_encoder(f'lm_encoder')\n",
    "cls_learner.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.728310</td>\n",
       "      <td>0.702305</td>\n",
       "      <td>0.527337</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683058</td>\n",
       "      <td>0.689539</td>\n",
       "      <td>0.570547</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651710</td>\n",
       "      <td>0.699294</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>0.712868</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learner.fit_one_cycle(4, 3e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.635676</td>\n",
       "      <td>0.735681</td>\n",
       "      <td>0.528219</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.644104</td>\n",
       "      <td>0.738805</td>\n",
       "      <td>0.536155</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.579515</td>\n",
       "      <td>0.783080</td>\n",
       "      <td>0.548501</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493157</td>\n",
       "      <td>0.828310</td>\n",
       "      <td>0.551146</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learner.freeze_to(-2)\n",
    "cls_learner.fit_one_cycle(4, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.458311</td>\n",
       "      <td>0.854441</td>\n",
       "      <td>0.549383</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.442702</td>\n",
       "      <td>0.886316</td>\n",
       "      <td>0.546737</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.417213</td>\n",
       "      <td>0.919518</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.412886</td>\n",
       "      <td>0.936245</td>\n",
       "      <td>0.552910</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learner.freeze_to(-3)\n",
    "cls_learner.fit_one_cycle(4, slice(5e-4/(2.6**4),5e-4), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.544974</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.394736</td>\n",
       "      <td>0.918777</td>\n",
       "      <td>0.541446</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.409746</td>\n",
       "      <td>0.947798</td>\n",
       "      <td>0.546737</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.405609</td>\n",
       "      <td>0.933561</td>\n",
       "      <td>0.552028</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.385743</td>\n",
       "      <td>0.937424</td>\n",
       "      <td>0.551146</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.391422</td>\n",
       "      <td>0.946191</td>\n",
       "      <td>0.545855</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learner.unfreeze()\n",
    "cls_learner.fit_one_cycle(6, slice(5e-5/(2.6**4),5e-5), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.388172</td>\n",
       "      <td>0.762243</td>\n",
       "      <td>0.622575</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.382363</td>\n",
       "      <td>0.768161</td>\n",
       "      <td>0.620811</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.766937</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.373341</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.620811</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.375050</td>\n",
       "      <td>0.774783</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.375479</td>\n",
       "      <td>0.769883</td>\n",
       "      <td>0.606702</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.369886</td>\n",
       "      <td>0.776303</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.374325</td>\n",
       "      <td>0.787260</td>\n",
       "      <td>0.615520</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.365175</td>\n",
       "      <td>0.778524</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.368064</td>\n",
       "      <td>0.785986</td>\n",
       "      <td>0.606702</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.372106</td>\n",
       "      <td>0.781850</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.366002</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.361924</td>\n",
       "      <td>0.778977</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.361054</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.358816</td>\n",
       "      <td>0.789724</td>\n",
       "      <td>0.615520</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.357182</td>\n",
       "      <td>0.819502</td>\n",
       "      <td>0.606702</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.366961</td>\n",
       "      <td>0.794313</td>\n",
       "      <td>0.611993</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.349990</td>\n",
       "      <td>0.806279</td>\n",
       "      <td>0.606702</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.363841</td>\n",
       "      <td>0.787805</td>\n",
       "      <td>0.611993</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.361152</td>\n",
       "      <td>0.794703</td>\n",
       "      <td>0.615520</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_learner.fit_one_cycle(20, slice(5e-5/(2.6**4),5e-5), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_type = \"two_class\"\n",
    "split_id = \"trial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learner.save(f'{split_type}_{split_id}_clas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test only on Canoicial SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-390ea4ae53dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_data_clas = TextClasDataBunch.from_df(path, train, test, bs=bs, tokenizer=tok, \n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Smiles'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqsar_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                               include_bos=False)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_classifier_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_clas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test_data_clas = TextClasDataBunch.from_df(path, train, test, bs=bs, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='Label', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
    "                                              include_bos=False)\n",
    "\n",
    "learner = text_classifier_learner(test_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "learner.load(f'{split_type}_{split_id}_clas', purge=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_scores(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Test on averaging prediction of canoicial and randomized SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_smiles_augmentation(df):\n",
    "    dist_aug = {col_name: [] for col_name in df}\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        dist_aug['Smiles'].append(randomize_smiles(df.iloc[i]['Smiles']))\n",
    "        dist_aug['Label'].append(df.iloc[i]['Label'])\n",
    "                     \n",
    "    return pd.DataFrame.from_dict(dist_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = torch.tensor(test['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "# Randomized SMILES Predictions\n",
    "for i in range(4):\n",
    "    np.random.seed(12*i)    \n",
    "    test_aug = test_smiles_augmentation(test)\n",
    "    \n",
    "    # model\n",
    "    test_data_clas = TextClasDataBunch.from_df(path, train, test_aug, bs=bs, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='Label', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
    "                                              include_bos=False)\n",
    "    learner = text_classifier_learner(test_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "    learner.load(f'{split_type}_{split_id}_clas', purge=False);\n",
    "    \n",
    "    \n",
    "    #get predictions\n",
    "    pred,lbl = learner.get_preds(ordered=True)\n",
    "    \n",
    "    preds.append(pred)\n",
    "\n",
    "# Canonical SMILES Predictions\n",
    "\n",
    "test_data_clas = TextClasDataBunch.from_df(path, train, test, bs=bs, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='Label', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
    "                                              include_bos=False)\n",
    "\n",
    "learner = text_classifier_learner(test_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "learner.load(f'{split_type}_{split_id}_clas', purge=False);\n",
    "\n",
    "\n",
    "pred,lbl = learner.get_preds(ordered=True)\n",
    "\n",
    "\n",
    "preds.append(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(preds)/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Averaging Predictions of Canoicial and Randomized SMILES: 0.908\n"
     ]
    }
   ],
   "source": [
    "avg_preds = sum(preds)/len(preds)\n",
    "print(f'Performance of Averaging Predictions of Canoicial and Randomized SMILES: {roc_auc_score(lbl, avg_preds[:,1], multi_class=\"ovo\"):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
