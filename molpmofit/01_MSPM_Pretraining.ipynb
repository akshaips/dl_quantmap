{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General-Domain MSPM Training\n",
    "\n",
    "In this notebook, we are going to train an universal molecular structure prediction model (MSPM) on **one million** compounds curated from [ChEMBL](https://www.ebi.ac.uk/chembl/). \n",
    "\n",
    "We use [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) as molecuar representation. SMILES is a type of textual represetnation for molecules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import sqlite3\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "from SmilesPE.spe2vec import Corpus\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from utils import *\n",
    "import torch\n",
    "print (torch.__version__)\n",
    "\n",
    "torch.cuda.set_device(0) #change to 0 if you only has one GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_workers = 8 # Number of threads to use\n",
    "train_percentage = 99 # Train and valid split percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get quantmap smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_old = sqlite3.connect('/mnt/external-images-pvc/quantmap/qm_chem_fix.sqlite') #OLD DATABASE\n",
    "co = conn_old.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rdkit warnings (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove rdkit warning\n",
    "\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data, convert to canonical smiles and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_canonical_smiles(molecule):\n",
    "    try:\n",
    "        canonical_smile = Chem.MolToSmiles(Chem.MolFromSmiles(molecule))\n",
    "    except:\n",
    "        canonical_smile = False\n",
    "    return canonical_smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.execute(\"select distinct smiles from stitch_chem;\")\n",
    "query_out = co.fetchall()\n",
    "\n",
    "quantmap_smiles = []\n",
    "for smile in query_out:\n",
    "    quantmap_smiles.append(smile[0])\n",
    "    \n",
    "\n",
    "p = Pool(Number_of_workers)\n",
    "canonical_quantmap_smiles = list(tqdm.tqdm(p.imap(to_canonical_smiles, quantmap_smiles), total=len(quantmap_smiles)))\n",
    "p.close()\n",
    "p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "out_csv = open(\"quantmap_canonical_smiles.csv\",\"w\")\n",
    "out_csv.write(\"Smiles,Canonical\\n\")\n",
    "loop = tqdm.tqdm(canonical_quantmap_smiles,total=len(canonical_quantmap_smiles),leave=False)\n",
    "for smile in loop:\n",
    "    if type(smile) == str:\n",
    "        out_csv.write(str(smile) + \",yes\\n\")\n",
    "out_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the previously saved canonical csv with shuffling\n",
    "csv_path = '/scratch-shared/akshai/workdir/molpmofit_method/quantmap_canonical_smiles.csv'\n",
    "smiles_df = pd.read_csv(csv_path).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training smiles = 16611236\n",
      "Number of validation smiles = 167791\n"
     ]
    }
   ],
   "source": [
    "ratio = int(smiles_df.shape[0] * (train_percentage/100))\n",
    "\n",
    "train_df = smiles_df.iloc[:ratio,:]\n",
    "valid_df = smiles_df.iloc[ratio:,:]\n",
    "\n",
    "train_df.to_pickle(\"train_canonical.pkl\")\n",
    "valid_df.to_pickle(\"valid_canonical.pkl\")\n",
    "\n",
    "print (\"Number of training smiles = \" + str(len(train_df)))\n",
    "print (\"Number of validation smiles = \" + str(len(valid_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle data\n",
    "train_df = pd.read_pickle(\"train_canonical.pkl\")\n",
    "valid_df = pd.read_pickle(\"valid_canonical.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smiles augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def randomize_smiles(smiles,random_smiles=[],iteration=5):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        ans = list(range(m.GetNumAtoms()))\n",
    "        np.random.shuffle(ans)\n",
    "        nm = Chem.RenumberAtoms(m,ans)\n",
    "        out_smiles = (Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False))\n",
    "    except:\n",
    "        return (False)\n",
    "    \n",
    "    if out_smiles not in random_smiles:\n",
    "        return out_smiles\n",
    "    else:\n",
    "        iteration -= 1\n",
    "        if iteration > 0:\n",
    "            out_smiles = randomize_smiles(smiles,random_smiles,iteration)\n",
    "            return out_smiles\n",
    "        return (False)\n",
    "    \n",
    "def augment_smiles(count,iteration,smiles):\n",
    "    random_smiles = []\n",
    "    for i in range(count):\n",
    "        if smiles != None:\n",
    "            out_smiles = randomize_smiles(smiles,random_smiles,iteration=iteration)\n",
    "            if out_smiles:\n",
    "                random_smiles.append(out_smiles)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    return random_smiles\n",
    "\n",
    "def unpack_and_write_list(smiles,filename):\n",
    "    for entry in smiles:\n",
    "        if type(entry) == list:\n",
    "            unpack_and_write_list(entry,filename)\n",
    "        else:\n",
    "            filename.write(entry + \",no\\n\")\n",
    "    \n",
    "def smiles_augmentation(df, N_rounds=1,iteration=5,data_set_type=\"train\"):\n",
    "    canonical_smiles = df['Smiles'].to_list()\n",
    "    \n",
    "    p = Pool(Number_of_workers)\n",
    "    func = partial(augment_smiles, N_rounds, iteration)\n",
    "    augmented_smiles = list(tqdm.tqdm(p.imap(func, canonical_smiles), total=len(canonical_smiles)))\n",
    "    p.close()\n",
    "    \n",
    "    print (\"Saving data\")\n",
    "    \n",
    "    filename = str(data_set_type) + \"_aug_canonical_smiles.csv\"\n",
    "    \n",
    "    aug_out = open(filename,\"w\")\n",
    "        \n",
    "    aug_out.write(\"Smiles,Canonical\\n\")\n",
    "    \n",
    "    unpack_and_write_list(augmented_smiles,filename=aug_out)\n",
    "    \n",
    "    unpack_and_write_list(canonical_smiles,filename=aug_out)\n",
    "    \n",
    "    aug_out.close()\n",
    "    \n",
    "    print (\"Saved data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "def read_shuffled_chunks(filepath: str, chunk_size: int,\n",
    "                        file_length: int, has_header=True):\n",
    "    \n",
    "    random.seed(1)\n",
    "    \n",
    "    header = 0 if has_header else None\n",
    "    first_data_idx = 1 if has_header else 0\n",
    "    # create index list\n",
    "    index_list = list(range(first_data_idx,file_length))\n",
    "\n",
    "    # shuffle the list in place\n",
    "    random.shuffle(index_list)\n",
    "\n",
    "    # iterate through the chunks and read them\n",
    "    n_chunks = math.ceil(file_length/chunk_size)\n",
    "    for i in range(n_chunks):\n",
    "\n",
    "        rows_to_keep = index_list[(i*chunk_size):((i+1)*chunk_size - 1)]\n",
    "        if has_header:\n",
    "            rows_to_keep += [0] # include the index row\n",
    "        # get the inverse selection\n",
    "        rows_to_skip = list(set(index_list) - set(rows_to_keep)) \n",
    "        yield pd.read_csv(filepath,skiprows=rows_to_skip, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16611236/16611236 [20:33<00:00, 13463.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n",
      "Saved data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167791/167791 [00:11<00:00, 13984.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n",
      "Saved data\n"
     ]
    }
   ],
   "source": [
    "number_of_augmentation = 1\n",
    "\n",
    "# Augmentation for training data\n",
    "train_data = smiles_augmentation(train_df,N_rounds=number_of_augmentation,iteration=100,data_set_type=\"train\")\n",
    "\n",
    "# Augmentation for validation data\n",
    "val_data = smiles_augmentation(valid_df,N_rounds=number_of_augmentation,iteration=100,data_set_type=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_expected_file_length = int((number_of_augmentation + 1 ) * train_df.shape[0])\n",
    "#valid_expected_file_length = int((number_of_augmentation + 1 ) * valid_df.shape[0])\n",
    "#train_chunk = read_shuffled_chunks(\"train_aug_canonical_smiles.csv\", 100,test_expected_file_length, has_header=True)\n",
    "#valid_chunk = read_shuffled_chunks(\"valid_aug_canonical_smiles.csv\", 100,valid_expected_file_length, has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_aug_canonical_smiles.csv\", header=0).sample(frac=1).reset_index(drop=True)\n",
    "valid_data = pd.read_csv(\"valid_aug_canonical_smiles.csv\", header=0).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from SmilesPE.learner import *\n",
    "from SmilesPE.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SMILES: 33558045\n"
     ]
    }
   ],
   "source": [
    "all_smiles = train_data['Smiles'].to_list()\n",
    "all_smiles.extend(valid_data['Smiles'].to_list())\n",
    "\n",
    "assert(len(train_data)+len(valid_data) == len(all_smiles))\n",
    "\n",
    "print('Number of SMILES:', len(all_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path('results')\n",
    "name = 'pretraining_new'\n",
    "path = result_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "mdl_path = path/'models'\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "token_path = 'results/tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting SMILES...\n",
      "3355010 unique Canonical SMILES\n",
      "Gettting Pair Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3355010' class='' max='3355010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3355010/3355010 01:41<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of unique characters & Reducing number of merge operations by: 490\n",
      "Unique characters: {'[Os+4]', '[Zn+]', 'N', '[Cl+]', '[Cu+2]', '[Re-2]', '[Os+2]', '[sH+]', '[Zr-2]', '[Cs+]', '8', '[Ne]', '[OH-]', '[S-2]', '[I+3]', '[Al-]', '[pH]', '[Ca+2]', '=', '[Ru-]', '[I+2]', '[Lu+3]', '[Tm]', '[O-]', '[Ti]', '[SH-]', '[Al-2]', '[Fm]', '[Cr+4]', '[Tc+]', '[Pa]', '[Cr+2]', '%29', '[B-]', '[SiH]', '[Pd+2]', '[Si+4]', '[Os-2]', '[Mn+2]', '[n+]', '[Si+3]', 'F', '[SH3+]', '[Y+3]', '[Li+]', '[In+3]', '[Xe]', '[al]', '[P+]', '[Fe-2]', 'B', '[Ir]', '[Au-]', '[Fe+2]', '[cH+]', '[Dy+3]', '%20', '[Al]', '[SnH3]', '[Lr]', '[Hf+2]', '[In-]', '[H-]', '[Se]', '[Ti+4]', '[PbH2]', '[NH]', '[Yb]', '[W+]', '[Gd+2]', '[Ag+]', '[Bk]', '[Ta+5]', '[Y]', '[BiH]', '[Sb+3]', '[At]', '[TeH]', '[ZrH]', '[U+2]', '[Co]', '[Pt+2]', '[Tl+]', '%10', '[NH2+]', '[Ir+2]', '[Ti-]', '[Ce+3]', '[Co+3]', '[NiH]', '[Ga]', '[K+]', '[NH4+]', '[Kr]', '[Th]', '[Os]', '[La]', '[SbH]', '[RuH]', '[Be+2]', '[Mo+4]', '[Cl]', '[AsH2]', '[V]', 'Br', '#', '[U]', '[Au+]', '[Ir+3]', '[As]', '[Al+]', '[Dy]', '[Sr]', '[si+2]', '[Ru+6]', '[H+]', '[Sb-]', '[Mn]', '[Rb+]', '%16', '[pH+]', '[SnH2]', '[Li-]', '%28', '(', '[AlH+2]', '[Sn]', '[BH4-]', '[Tc+6]', '%25', '%24', '[PH4+]', '[C+]', '[PH-]', '[In+]', '[Ce]', '[NH+]', '[GeH4]', '[Ti+]', '%23', '2', '[Sb+]', '[TlH]', '3', '[Cm]', '-', '[N-]', '[Se-]', '[Al+3]', '[RuH+]', '[Ba]', '[Pt]', '[oH+]', '[Re]', '[O]', '[GaH3]', '[Cl+3]', '[nH]', '[OH+]', '.', '[Sr+2]', '[Ti+2]', '[Pm]', '[SiH-]', '[Si]', '[Mg+2]', 'C', '[Ru+4]', '[Se+]', '[Ho]', '[Ru+2]', '[Cd]', '[IH2+]', '[Hg+2]', '[Zn-]', 'S', '[K]', '[Cu+]', '[Pt+4]', '[Tc]', '[Ni-]', '[Zn-2]', '[se]', '[He]', '%12', '[NH2-]', '[Co+]', '[p-]', '[Rh+2]', '[Se-2]', '[Ru+5]', '[Eu+2]', '[Bi+2]', '[MoH2]', '[Mn+3]', '[Lu]', 'P', '[Ba+2]', '[Pd+]', '[N]', '[Sn-]', '[Ta+2]', '[SiH2]', '[Au+3]', '[Nd]', '[Nb+3]', '[CH3+]', '[Co-2]', '[PH3+]', '[C-]', '[Eu+3]', '[Hg-]', '[Nb]', '[Ti+3]', '[Hf+]', '[GeH3]', '[Zr+3]', '[Ir-2]', '[Pd]', '[Sn+2]', '[Br-]', '[Br+3]', '[te]', '[OsH]', '[As+3]', '[Zn]', '[Os+6]', '[Be]', '[BH]', '%21', '[O-2]', '[Na+]', '4', '[Ru+3]', '[Fr]', '[Ti-2]', '[Te]', '[Tc+2]', '[PH2-]', '[AsH]', 'b', '[WH2]', '[Ag]', '[GaH2]', '[AtH]', '%17', '[Ir+]', '%14', '[Pt-]', '6', '[Np]', '[Pb+2]', '[Ru]', '[Ho+3]', '[Ru+]', '[OH3+]', '[Bi+]', '[ClH+]', '[PbH2+2]', '[Mg]', '[As+]', '[W]', '5', '[Bi+3]', '[NH3+]', '[Er]', '[Nd+3]', '[S-]', '[ReH]', '[SH]', '[Rn]', '[La+3]', '[PtH+]', '[RuH2]', '[RuH3]', '[Te+4]', '[Ca]', '[Ru+8]', '[F-]', '[PbH3]', '[Fe+5]', '[Sn+]', '[Mg+]', '[Cr+]', '[Ir-3]', '[Sb]', '[Rh+3]', '[SnH]', '[CH2]', '[Cu]', '[W+2]', '%22', '[Ru-3]', '[RhH]', '[CH]', '[InH3]', '[Si+2]', '[C]', '[H]', '[Es]', '[O+]', '[Si+]', '%19', 'I', '[GeH]', '[SiH4]', '%11', '[Er+3]', '[CH3-]', '[Co-]', '[BH2]', '[cH-]', '[Ni+3]', '[siH]', '[Zr+2]', 'c', '[Hg]', '[si]', '[Ga-]', '[Cu-2]', '[Nb+2]', '[Th+2]', '[Sn+4]', '[Yb+2]', '[I]', '[BiH2]', '[ClH2+]', '[Br+2]', '[PH]', 's', '[se+]', '[Zr]', '[Sm]', '[AlH3]', '[IH]', '[b-]', 'Cl', '[Cr+5]', '[Cr]', '[S+]', '[I-]', '[PH2+]', '[SH+]', '[SiH3]', '[Ta]', '[Yb+3]', '[GeH2]', '[CuH]', '[Bi]', '[N+]', '[Hg+]', '[c+]', '%15', '[Na]', '[Ni+2]', '[CH-]', '[Ni]', '[PbH]', '[As-]', '[Ta-2]', '[Pu]', '[Br]', '[BH2-]', '[Pd-]', '[P-]', '[Fe+]', '[Pb]', '[Pr]', '[Cl-]', '[Gd+3]', '[V+2]', '[Tb]', '[Zr-]', '[Sc]', '[Hg-2]', '[Nb-]', '[Ga+3]', '[Al-3]', '[RhH2]', '[c-]', '%27', '[Mo+]', '[o+]', '[P-3]', '[Cr+3]', '1', '[n-]', '[Si-]', '[Al+2]', '[Sc+3]', '[Nb+5]', '[Th+4]', '[Ni+]', '[Tl]', '[CH2+]', '[Zr+4]', '%13', '[Fe-]', '[Os+8]', '[V+4]', '[Zn+2]', '[Cl+2]', '[In]', '[SeH-]', '[Cd+2]', '[PH+]', '[Tb+3]', '[Pt+]', '[Rh]', '[Ge]', '[SnH4]', '[Ce+4]', 'p', '[Fe+3]', '7', '[As+5]', '[Po]', '[Se+6]', '[BH-]', '[Sm+3]', '[Mo]', '[OH2+]', '[NH-]', 'O', '%30', '[Cf]', '[Pt-2]', '[s+]', '[Hf+4]', '[AlH]', '[nH+]', '[Rh+]', '[Pr+3]', '[HH]', '9', '[Pd-2]', '[CH+]', '[Rb]', 'o', '[Gd]', '%18', '[IH+]', '[SeH]', '[Fe+6]', '[Cs]', '[I+]', '[Mo+2]', '[Eu]', '[Hf]', '[Rh-]', '[Au]', '[Md]', '[SH2+]', '%26', '[AlH2]', '[Mn+]', '[c]', '[Tl+3]', '[Cr-2]', '[Os+7]', '[Tc+4]', '[BH3-]', '[Mo+3]', '[S]', '[Te+]', '[Tm+3]', '[Os+]', '[No]', '[p+]', '[CH2-]', '[AsH-]', '[Sn+3]', '[Ac]', '[Fe]', '[Co+2]', '[InH]', '[IrH+2]', '[Tc+7]', '[Tc+5]', '[Am]', '[B]', '[P]', ')', '[Li]', 'n', '[Fe+4]', '[Tc+3]', '[Cr+6]', '[Ar]', '[InH2]'}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = codecs.open(token_path, 'w')\n",
    "learn_SPE(all_smiles, output, 30000, min_frequency=2000, augmentation=0, verbose=False, total_symbols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC [N+](C)(C) Cc1 I cc c Br cc1 [Ce+3] [Fe+3] ['C', 'C', '[N+]', '(', 'C', ')', '(', 'C', ')', 'C', 'c', '1', 'I', 'c', 'c', 'c', 'Br', 'c', 'c', '1', '[Ce+3]', '[Fe+3]']\n"
     ]
    }
   ],
   "source": [
    "spe_vob= codecs.open(token_path)\n",
    "spe = SPE_Tokenizer(spe_vob)\n",
    "\n",
    "smi = 'CC[N+](C)(C)Cc1IcccBrcc1[Ce+3][Fe+3]'\n",
    "print (spe.tokenize(smi),atomwise_tokenizer(smi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"train_canonical.pkl\").sample(frac=1).reset_index(drop=True)\n",
    "valid_data = pd.read_pickle(\"valid_canonical.pkl\").sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#train_data = train_data.iloc[:int(len(train_data) * 0.0005),:]\n",
    "#valid_data = valid_data.iloc[:int(len(valid_data) * 0.0005),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16611236"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang = 'en'):\n",
    "        self.lang = lang\n",
    "        \n",
    "    def tokenizer(self, smiles):        \n",
    "        \n",
    "        tokens = atomwise_tokenizer(smiles)\n",
    "        return tokens\n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(partial(MolTokenizer), n_cpus=8, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/anaconda3/envs/molpmofit/lib/python3.7/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 35s, sys: 21min 37s, total: 29min 12s\n",
      "Wall time: 33min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bs = 512 # batch size\n",
    "\n",
    "data = TextLMDataBunch.from_df(path, train_data, valid_data, bs=bs, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols=0, max_vocab=60000, include_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4 ) c ( F ) c c 3 c 1 = O ) O C C 2 C F C C c 1 c c c ( N C ( = O ) c 2 c c c ( N C ( = O ) c 3 c c c c c 3 ) c c 2 ) c c 1 C O c 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>( N C ( = O ) C ( c 2 c c c 3 c ( c 2 ) O C O 3 ) N ( C c 2 c c c ( C ) c c 2 ) C ( = O ) C c 2 c c c c c 2 ) c ( O C ) c 1 C c 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1 O C C C c 1 n c ( S C C ( O ) C O C c 2 c c c ( O C ) c c 2 ) n c ( N ) c 1 - c 1 c c c ( Cl ) c c 1 C c 1 c c c ( - n 2 c ( C c 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3 ) n c n 2 ) c 1 C C c 1 c c c c ( N C c 2 c c c c c 2 O C C # N ) c 1 C C ( C ) O c 1 c c c ( N ( C C ( = O ) N C C C c 2 c c c c (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1 n c ( N 2 C C O C C 2 ) s c 1 N = O Cl c 1 c c c ( N 2 C C C ( [NH2+] C 3 C O c 4 c c c c c 4 C 3 ) C C 2 ) c c 1 N C 1 = C ( c 2 c c c c ( C (</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the databunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 16611236)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 16611236)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.save(f'{name}_databunch')\n",
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the databunch generated in last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.61\n"
     ]
    }
   ],
   "source": [
    "from fastai.basic_data import load_data\n",
    "import fastai\n",
    "print (fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512 # batch size\n",
    "data_lm = load_data(path, f'{name}_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the [model](https://docs.fast.ai/text.learner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = language_model_learner(data_lm, AWD_LSTM, drop_mult = 1.,pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(560, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(560, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=560, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the model. I trained the model on a single **Quadro P4000** GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [1/10 3:34:29<32:10:22]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.711222</td>\n",
       "      <td>0.663281</td>\n",
       "      <td>0.770622</td>\n",
       "      <td>3:34:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1325' class='' max='20944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      6.33% [1325/20944 13:27<3:19:09 0.6838]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-604391554058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/molpmofit/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m~/anaconda3/envs/molpmofit/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/molpmofit/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/molpmofit/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/molpmofit/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/molpmofit/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 3e-3\n",
    "lr *= bs/48  # Scale learning rate by batch size\n",
    "\n",
    "learner.unfreeze()\n",
    "learner.fit_one_cycle(10, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both the weights and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fns = [f'{name}_wt', f'{name}_vocab']\n",
    "\n",
    "learner.save(lm_fns[0], with_opt=False)\n",
    "learner.data.vocab.save(mdl_path/(lm_fns[1] + '.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
